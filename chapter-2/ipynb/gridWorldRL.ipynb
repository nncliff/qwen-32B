{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e9950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size: int = 5, start =(0, 0), goal=(4, 4)):\n",
    "        self.size = size\n",
    "        self.start = start  # Start at specified start position\n",
    "        self.goal = goal  # Goal at specified goal position\n",
    "        self.state = start\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action: str):\n",
    "        x, y = self.state\n",
    "        if action == 0:  # up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # down\n",
    "            x = min(self.size - 1, x + 1)\n",
    "        elif action == 2:  # left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # right\n",
    "            y = min(self.size - 1, y + 1)\n",
    "        \n",
    "        self.state = (x, y)\n",
    "        \n",
    "        done = self.state == self.goal\n",
    "        \n",
    "        return self.state, done\n",
    "\n",
    "    def get_possible_actions(self):\n",
    "        return [0, 1, 2, 3]  # up, down, left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: GridWorld):\n",
    "        self.env = env\n",
    "        self.q_table = np.zeros((env.size, env.size, 4))  # Q-values for each state-action pair\n",
    "        self.alpha = 0.1  # learning rate\n",
    "        self.gamma = 0.9  # discount factor\n",
    "        self.epsilon = 0.1  # exploration rate\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.env.get_possible_actions())  # Explore\n",
    "        else:\n",
    "            x, y = state\n",
    "            return np.argmax(self.q_table[x, y])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "        predict = self.q_table[x, y, action]\n",
    "        target = reward + self.gamma * np.max(self.q_table[next_x, next_y])\n",
    "        self.q_table[x, y, action] += self.alpha * (target - predict)\n",
    "\n",
    "    def predict_next_state(self, state, action):\n",
    "        x, y = state\n",
    "        if action == 0:  # up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # down\n",
    "            x = min(self.env.size - 1, x + 1)\n",
    "        elif action == 2:  # left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # right\n",
    "            y = min(self.env.size - 1, y + 1)\n",
    "        return (x, y)\n",
    "\n",
    "    def self_supervised_reward(self, state, action, next_state):\n",
    "        # Predict next state and compute error\n",
    "        predicted_next_state = self.predict_next_state(state, action)\n",
    "        # Compute L1 error\n",
    "        error = np.sum(np.abs(np.array(predicted_next_state) - np.array(next_state)))\n",
    "        return -error  # Negative error as reward, smaller error -> higher reward\n",
    "\n",
    "    def train(self, episodes: int = 1000):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, done = self.env.step(action)\n",
    "                reward = self.self_supervised_reward(state, action, next_state)\n",
    "                self.learn(state, action, reward, next_state)\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = Agent(env)\n",
    "\n",
    "agent.train()\n",
    "\n",
    "# print learned Q-table\n",
    "for i in range(env.size):\n",
    "    for j in range(env.size):\n",
    "        print(f\"State ({i},{j}): {agent.q_table[i,j]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
