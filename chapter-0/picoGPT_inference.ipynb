{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b0cc85f",
      "metadata": {},
      "source": [
        "# はじめに / Credit\n",
        "このNotebookの実装コードは、Jay Mody氏による「NumPyのみで書かれたGPT-2実装」である **[picoGPT](https://github.com/jaymody/picoGPT)** をベースにしています。\n",
        "\n",
        "本シリーズの教育的な目的のために、オリジナルのPythonスクリプトをJupyter Notebook形式に再構成（リファクタリング）し、日本語による詳細なコード解説と、中間変数の形状(Shape)を確認するためのステップを追加しました。\n",
        "\n",
        "* **Original Repository**: [jaymody/picoGPT](https://github.com/jaymody/picoGPT)\n",
        "* **Original Author**: Jay Mody\n",
        "* **License**: MIT License\n",
        "* **Copyright**: (c) 2023 Jay Mody\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ff0c66",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-0/picoGPT_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a12e2da",
      "metadata": {
        "id": "8a12e2da"
      },
      "source": [
        "# PicoGPT 推論ノートブック\n",
        "\n",
        "このノートブックでは、picoGPT実装を使用してGPT-2をロードし、推論を実行する方法を説明します。\n",
        "\n",
        "picoGPTは純粋なNumPyで実装されたGPT-2の最小実装であり、Transformerベースの言語モデルのコア概念を理解しやすくしています。\n",
        "\n",
        "**特徴:**\n",
        "- 事前学習済みGPT-2の重みをロード\n",
        "- BPEトークナイゼーション\n",
        "- 貪欲法（グリーディデコーディング）によるテキスト生成"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5f4db3",
      "metadata": {
        "id": "ed5f4db3"
      },
      "source": [
        "## 1. セットアップとインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c42a6bd1",
      "metadata": {
        "id": "c42a6bd1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from functools import lru_cache\n",
        "\n",
        "import numpy as np\n",
        "import regex\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b6cf84",
      "metadata": {
        "id": "d1b6cf84"
      },
      "source": [
        "## 2. BPEトークナイザー（エンコーダー）\n",
        "\n",
        "GPT-2トークナイザーはByte Pair Encoding（BPE）を使用してテキストをトークンに変換します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "60f98bd1",
      "metadata": {
        "id": "60f98bd1"
      },
      "outputs": [],
      "source": [
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8 + n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    \"\"\"BPE Encoder/Decoder for GPT-2.\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.errors = errors\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "        self.pat = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = \" \".join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encode text to token IDs.\"\"\"\n",
        "        bpe_tokens = []\n",
        "        for token in regex.findall(self.pat, text):\n",
        "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"Decode token IDs back to text.\"\"\"\n",
        "        text = \"\".join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
        "        return text\n",
        "\n",
        "\n",
        "def get_encoder(model_name, models_dir):\n",
        "    \"\"\"Load the BPE encoder from files.\"\"\"\n",
        "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
        "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40db22d5",
      "metadata": {
        "id": "40db22d5"
      },
      "source": [
        "## 3. モデルロードユーティリティ\n",
        "\n",
        "OpenAIからGPT-2の重みをダウンロードし、TensorFlowチェックポイントからロードする関数群です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f2187272",
      "metadata": {
        "id": "f2187272"
      },
      "outputs": [],
      "source": [
        "def download_gpt2_files(model_size, model_dir):\n",
        "    \"\"\"Download GPT-2 model files from OpenAI.\"\"\"\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "    for filename in [\n",
        "        \"checkpoint\",\n",
        "        \"encoder.json\",\n",
        "        \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\",\n",
        "        \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\",\n",
        "        \"vocab.bpe\",\n",
        "    ]:\n",
        "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
        "        r.raise_for_status()\n",
        "\n",
        "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
        "            file_size = int(r.headers[\"content-length\"])\n",
        "            chunk_size = 1000\n",
        "            with tqdm(\n",
        "                ncols=100,\n",
        "                desc=\"Fetching \" + filename,\n",
        "                total=file_size,\n",
        "                unit_scale=True,\n",
        "                unit=\"b\",\n",
        "            ) as pbar:\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(chunk_size)\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n",
        "    \"\"\"Load GPT-2 parameters from TensorFlow checkpoint.\"\"\"\n",
        "    def set_in_nested_dict(d, keys, val):\n",
        "        if not keys:\n",
        "            return val\n",
        "        if keys[0] not in d:\n",
        "            d[keys[0]] = {}\n",
        "        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n",
        "        return d\n",
        "\n",
        "    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n",
        "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
        "        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
        "        name = name[len(\"model/\") :]\n",
        "        if name.startswith(\"h\"):\n",
        "            m = re.match(r\"h([0-9]+)/(.*)\", name)\n",
        "            n = int(m[1])\n",
        "            sub_name = m[2]\n",
        "            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n",
        "        else:\n",
        "            set_in_nested_dict(params, name.split(\"/\"), array)\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def load_encoder_hparams_and_params(model_size, models_dir):\n",
        "    \"\"\"Load encoder, hyperparameters, and model parameters.\"\"\"\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    if not tf_ckpt_path:  # download files if necessary\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        download_gpt2_files(model_size, model_dir)\n",
        "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "\n",
        "    encoder = get_encoder(model_size, models_dir)\n",
        "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
        "\n",
        "    return encoder, hparams, params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f84949bb",
      "metadata": {
        "id": "f84949bb"
      },
      "source": [
        "## 4. GPT-2モデルコンポーネント\n",
        "\n",
        "これらは純粋なNumPyで実装されたGPT-2アーキテクチャのコアビルディングブロックです。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9920bff",
      "metadata": {},
      "source": [
        "### GELU（ガウス誤差線形ユニット）\n",
        "\n",
        "GELUはGPT-2（およびBERT、GPT-3などの多くの最新Transformer）で使用される活性化関数です。0で急激なカットオフを持つReLUとは異なり、GELUは滑らかな確率的ゲーティングメカニズムを提供します。\n",
        "\n",
        "**正確な定義:**\n",
        "\n",
        "$$\\text{GELU}(x) = x \\cdot \\Phi(x) = x \\cdot P(X \\leq x)$$\n",
        "\n",
        "ここで$\\Phi(x)$は標準正規分布の累積分布関数（CDF）です。\n",
        "\n",
        "**近似（実際に使用される）:**\n",
        "\n",
        "正確なCDFの計算はコストが高いため、GPT-2では$\\tanh$に基づく高速近似を使用します：\n",
        "\n",
        "$$\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)\\right)\\right)$$\n",
        "\n",
        "**なぜReLUではなくGELUか？**\n",
        "- **滑らか**: すべての点で微分可能（0でキンクなし）\n",
        "- **非単調**: 小さな負の値が小さな正の出力を持つことがある\n",
        "- **確率的解釈**: 入力値に依存する確率でベルヌーイマスクを乗算すると見なすことができる"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7474f4e1",
      "metadata": {},
      "source": [
        "### ソフトマックス\n",
        "\n",
        "ソフトマックスは実数のベクトル（ロジット）を確率分布に変換します。アテンションメカニズムと最終出力層で使用されます。\n",
        "\n",
        "**定義:**\n",
        "\n",
        "ベクトル$\\mathbf{x} = [x_1, x_2, ..., x_n]$に対して：\n",
        "\n",
        "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
        "\n",
        "**性質:**\n",
        "- 出力値は$(0, 1)$の範囲\n",
        "- すべての出力の合計は1: $\\sum_i \\text{softmax}(x_i) = 1$\n",
        "- 順序を保存: $x_i > x_j$ならば$\\text{softmax}(x_i) > \\text{softmax}(x_j)$\n",
        "\n",
        "**数値安定性の問題:**\n",
        "\n",
        "$e^{x_i}$を直接計算すると、大きな$x$でオーバーフローが発生する可能性があります。解決策は最大値を引くことです：\n",
        "\n",
        "$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(\\mathbf{x})}}{\\sum_{j=1}^{n} e^{x_j - \\max(\\mathbf{x})}}$$\n",
        "\n",
        "これは数学的に同等（$e^{-\\max(\\mathbf{x})}$が打ち消し合う）ですが、最大の指数が$e^0 = 1$になるためオーバーフローを防ぎます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a163d9b",
      "metadata": {},
      "source": [
        "### レイヤー正規化\n",
        "\n",
        "レイヤー正規化は特徴次元全体で活性化を正規化することで訓練を安定させます。バッチ正規化（バッチ全体で正規化）とは異なり、レイヤー正規化は個々のサンプルに対して機能します。\n",
        "\n",
        "**定義:**\n",
        "\n",
        "次元$d$の入力ベクトル$\\mathbf{x} = [x_1, x_2, ..., x_d]$に対して：\n",
        "\n",
        "$$\\text{LayerNorm}(\\mathbf{x}) = \\gamma \\cdot \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
        "\n",
        "ここで：\n",
        "- $\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i$ は平均\n",
        "- $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2$ は分散\n",
        "- $\\gamma$（スケール）と$\\beta$（シフト）は学習可能なパラメータ\n",
        "- $\\epsilon$は数値安定性のための小さな定数（例：$10^{-5}$）\n",
        "\n",
        "**なぜレイヤー正規化か？**\n",
        "- **勾配の安定化**: 活性化が大きくなりすぎたり小さくなりすぎたりするのを防ぐ\n",
        "- **より深いネットワークを可能に**: 多くの層を持つTransformerの訓練に不可欠\n",
        "- **バッチサイズに依存しない**: 訓練と推論で同じように動作\n",
        "\n",
        "**GPT-2での使用:**\n",
        "- アテンションとFFNの**前**に適用（Pre-LNアーキテクチャ）\n",
        "- 各Transformerブロックには2つのレイヤー正規化がある：`ln_1`（アテンション前）と`ln_2`（FFN前）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4609d1c3",
      "metadata": {},
      "source": [
        "### 線形層（全結合層）\n",
        "\n",
        "線形変換はニューラルネットワークの基本的なビルディングブロックです。全結合層、密層、またはアフィン変換とも呼ばれます。\n",
        "\n",
        "**定義:**\n",
        "\n",
        "入力$\\mathbf{x} \\in \\mathbb{R}^{d_{in}}$、重み行列$\\mathbf{W} \\in \\mathbb{R}^{d_{in} \\times d_{out}}$、バイアス$\\mathbf{b} \\in \\mathbb{R}^{d_{out}}$に対して：\n",
        "\n",
        "$$\\text{Linear}(\\mathbf{x}) = \\mathbf{x} \\mathbf{W} + \\mathbf{b}$$\n",
        "\n",
        "**形状変換:**\n",
        "- 入力: $[m, d_{in}]$（$m$個のベクトルのバッチ）\n",
        "- 重み: $[d_{in}, d_{out}]$\n",
        "- バイアス: $[d_{out}]$\n",
        "- 出力: $[m, d_{out}]$\n",
        "\n",
        "**GPT-2での線形層の用途:**\n",
        "- **QKV射影**（`c_attn`）: 入力をクエリ、キー、バリューベクトルに射影\n",
        "- **出力射影**（`c_proj`）: アテンション出力を埋め込み次元に射影\n",
        "- **FFN上方射影**（`c_fc`）: $d_{model}$から$4 \\cdot d_{model}$に拡張\n",
        "- **FFN下方射影**（`c_proj`）: $d_{model}$に圧縮\n",
        "\n",
        "**注:** NumPy/Pythonの`@`演算子は行列乗算を実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "540fdfa5",
      "metadata": {
        "id": "540fdfa5"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    \"\"\"Gaussian Error Linear Unit activation function.\"\"\"\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Numerically stable softmax function.\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "\n",
        "def layer_norm(x, g, b, eps: float = 1e-5):\n",
        "    \"\"\"Layer normalization.\"\"\"\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
        "    return g * x + b  # scale and offset with gamma/beta params\n",
        "\n",
        "\n",
        "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
        "    \"\"\"Linear transformation.\"\"\"\n",
        "    return x @ w + b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "768eef82",
      "metadata": {},
      "source": [
        "### フィードフォワードネットワーク（FFN / MLP）\n",
        "\n",
        "フィードフォワードネットワーク（MLPまたは位置ごとのフィードフォワードとも呼ばれる）は、シーケンス内の各位置に独立して適用されます。GELUの活性化を挟んだ2つの線形変換で構成されます。\n",
        "\n",
        "**定義:**\n",
        "\n",
        "$$\\text{FFN}(\\mathbf{x}) = \\text{Linear}_2(\\text{GELU}(\\text{Linear}_1(\\mathbf{x})))$$\n",
        "\n",
        "より明示的に：\n",
        "\n",
        "$$\\text{FFN}(\\mathbf{x}) = \\text{GELU}(\\mathbf{x} \\mathbf{W}_1 + \\mathbf{b}_1) \\mathbf{W}_2 + \\mathbf{b}_2$$\n",
        "\n",
        "**アーキテクチャ:**\n",
        "```\n",
        "入力 [n_seq, d_model]\n",
        "    ↓\n",
        "線形（上方射影）: d_model → 4 × d_model\n",
        "    ↓\n",
        "GELU活性化\n",
        "    ↓\n",
        "線形（下方射影）: 4 × d_model → d_model\n",
        "    ↓\n",
        "出力 [n_seq, d_model]\n",
        "```\n",
        "\n",
        "**なぜ4倍拡張か？**\n",
        "- 中間次元は通常$4 \\times d_{model}$（例：GPT-2 smallでは768 → 3072）\n",
        "- この「ボトルネック」設計により、ネットワークはより豊かな表現を学習できる\n",
        "- 拡張により非線形変換のためのより多くの容量を提供\n",
        "\n",
        "**GPT-2での使用:**\n",
        "- `c_fc`: 上方射影の重み（次元を拡張）\n",
        "- `c_proj`: 下方射影の重み（圧縮）\n",
        "- 各Transformerブロックでレイヤー正規化の後に適用"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a78d33",
      "metadata": {},
      "source": [
        "### スケールドドットプロダクトアテンション\n",
        "\n",
        "アテンションは、各出力を生成する際に入力の関連部分に焦点を当てることを可能にするコアメカニズムです。クエリとキー間の類似度によって重みが決定される値の重み付き和を計算します。\n",
        "\n",
        "**定義:**\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\text{mask}\\right) V$$\n",
        "\n",
        "ここで：\n",
        "- $Q \\in \\mathbb{R}^{n_q \\times d_k}$ — クエリ行列（探しているもの）\n",
        "- $K \\in \\mathbb{R}^{n_k \\times d_k}$ — キー行列（マッチングする対象）\n",
        "- $V \\in \\mathbb{R}^{n_k \\times d_v}$ — バリュー行列（取得するもの）\n",
        "- $d_k$ — キーの次元（スケーリングに使用）\n",
        "- mask — 将来のトークンへのアテンションを防ぐ因果マスク\n",
        "\n",
        "**ステップバイステップの分解:**\n",
        "\n",
        "1. **アテンションスコアの計算**: $QK^T$は各クエリとキー間の類似度を与える\n",
        "2. **スケーリング**: 大きな$d_k$でのソフトマックス飽和を防ぐため$\\sqrt{d_k}$で割る\n",
        "3. **マスク**: アテンションすべきでない位置に$-\\infty$（または$-10^{10}$）を加える\n",
        "4. **ソフトマックス**: スコアを確率に変換（行の合計は1）\n",
        "5. **重み付き和**: $V$を掛けて出力を得る\n",
        "\n",
        "**なぜ$\\sqrt{d_k}$でスケーリングするのか？**\n",
        "\n",
        "スケーリングなしでは、$d_k$が大きい場合、内積$q \\cdot k$の大きさが増大し、ソフトマックスを非常に小さな勾配を持つ領域に押し込みます。$\\sqrt{d_k}$でスケーリングすることで分散を安定に保ちます。\n",
        "\n",
        "**因果マスク（GPT用）:**\n",
        "\n",
        "自己回帰モデルでは、位置$i$が位置$\\leq i$にのみアテンションできるように下三角マスクを使用します：\n",
        "\n",
        "$$\\text{mask}_{ij} = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e00f7fd4",
      "metadata": {},
      "source": [
        "### マルチヘッドアテンション（MHA）\n",
        "\n",
        "マルチヘッドアテンションは、それぞれ異なる学習済み射影を持つ複数のアテンション操作を並列に実行します。これにより、モデルは異なる表現サブスペースからの情報に同時にアテンションを向けることができます。\n",
        "\n",
        "**定義:**\n",
        "\n",
        "$$\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O$$\n",
        "\n",
        "各ヘッドは：\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)$$\n",
        "\n",
        "**アーキテクチャ:**\n",
        "```\n",
        "入力 X [n_seq, d_model]\n",
        "    ↓\n",
        "線形射影 → Q, K, V [n_seq, 3 × d_model]\n",
        "    ↓\n",
        "n_headヘッドに分割 → 各ヘッドの次元 d_k = d_model / n_head\n",
        "    ↓\n",
        "各ヘッドで並列アテンション（因果マスク付き）\n",
        "    ↓\n",
        "ヘッドを連結 → [n_seq, d_model]\n",
        "    ↓\n",
        "出力射影 → [n_seq, d_model]\n",
        "```\n",
        "\n",
        "**なぜ複数のヘッドか？**\n",
        "\n",
        "- **異なるアテンションパターン**: 各ヘッドは異なる側面（例：構文vs意味論、近くvs遠くのトークン）に焦点を当てることを学習できる\n",
        "- **より豊かな表現**: 同じ総パラメータ数の単一アテンションよりも表現力が高い\n",
        "- **並列計算**: すべてのヘッドが独立して計算され、効率的な並列化が可能\n",
        "\n",
        "**GPT-2の設定:**\n",
        "| モデル | $d_{model}$ | $n_{head}$ | $d_k = d_{model}/n_{head}$ |\n",
        "|-------|-------------|------------|----------------------------|\n",
        "| 124M  | 768         | 12         | 64                         |\n",
        "| 355M  | 1024        | 16         | 64                         |\n",
        "| 774M  | 1280        | 20         | 64                         |\n",
        "| 1558M | 1600        | 25         | 64                         |\n",
        "\n",
        "**コード内での表現:**\n",
        "- `c_attn`: 結合QKV射影の重み $[d_{model}, 3 \\times d_{model}]$\n",
        "- `c_proj`: 出力射影の重み $[d_{model}, d_{model}]$\n",
        "- `np.split(..., 3)`: 射影をQ, K, Vに分割\n",
        "- `np.split(..., n_head)`: 各々を複数のヘッドに分割"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7fae8700",
      "metadata": {
        "id": "7fae8700"
      },
      "outputs": [],
      "source": [
        "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    \"\"\"Feed-forward network (MLP) in transformer.\"\"\"\n",
        "    # project up\n",
        "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
        "\n",
        "    # project back down\n",
        "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
        "    \"\"\"Scaled dot-product attention.\"\"\"\n",
        "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
        "\n",
        "\n",
        "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    \"\"\"Multi-head attention.\"\"\"\n",
        "    # qkv projection\n",
        "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
        "\n",
        "    # split into heads\n",
        "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # causal mask to hide future inputs from being attended to\n",
        "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
        "\n",
        "    # perform attention over each head\n",
        "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # merge heads\n",
        "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
        "\n",
        "    # out projection\n",
        "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f27f058",
      "metadata": {},
      "source": [
        "### Transformerブロック\n",
        "\n",
        "Transformerブロックは、GPT-2の基本的な繰り返しユニットです。各ブロックは、自己アテンション（位置間で情報を混合）とフィードフォワードネットワーク（各位置を独立して処理）を組み合わせ、残差接続とレイヤー正規化で接続されています。\n",
        "\n",
        "**定義（Pre-LNアーキテクチャ）:**\n",
        "\n",
        "GPT-2は**Pre-LayerNorm**バリアントを使用し、正規化は各サブレイヤーの*前*に適用されます：\n",
        "\n",
        "$$\\mathbf{x} = \\mathbf{x} + \\text{MHA}(\\text{LayerNorm}(\\mathbf{x}))$$\n",
        "$$\\mathbf{x} = \\mathbf{x} + \\text{FFN}(\\text{LayerNorm}(\\mathbf{x}))$$\n",
        "\n",
        "**アーキテクチャ図:**\n",
        "```\n",
        "入力 x [n_seq, d_model]\n",
        "    │\n",
        "    ├───────────────────────────┐\n",
        "    ↓                           │ (残差)\n",
        "LayerNorm (ln_1)                │\n",
        "    ↓                           │\n",
        "マルチヘッドアテンション         │\n",
        "    ↓                           │\n",
        "    + ←─────────────────────────┘\n",
        "    │\n",
        "    ├───────────────────────────┐\n",
        "    ↓                           │ (残差)\n",
        "LayerNorm (ln_2)                │\n",
        "    ↓                           │\n",
        "フィードフォワードネットワーク   │\n",
        "    ↓                           │\n",
        "    + ←─────────────────────────┘\n",
        "    ↓\n",
        "出力 [n_seq, d_model]\n",
        "```\n",
        "\n",
        "**主要コンポーネント:**\n",
        "\n",
        "| コンポーネント | 目的 | パラメータ |\n",
        "|-----------|---------|------------|\n",
        "| `ln_1` | アテンション前の正規化 | $\\gamma_1, \\beta_1$ |\n",
        "| `attn` | マルチヘッド自己アテンション | $W^Q, W^K, W^V, W^O$ |\n",
        "| `ln_2` | FFN前の正規化 | $\\gamma_2, \\beta_2$ |\n",
        "| `mlp` | 位置ごとのフィードフォワード | $W_1, b_1, W_2, b_2$ |\n",
        "\n",
        "**なぜ残差接続か？**\n",
        "\n",
        "`+`操作は、ResNetで導入された**残差（スキップ）接続**です：\n",
        "\n",
        "$$\\text{output} = \\text{input} + F(\\text{input})$$\n",
        "\n",
        "利点：\n",
        "- **勾配の流れ**: 勾配がスキップ接続を通じて直接流れ、深いネットワークでの勾配消失を防ぐ\n",
        "- **恒等写像**: $F$がゼロを出力する場合、レイヤーは恒等関数になり、最適化が容易\n",
        "- **インクリメンタル学習**: 各サブレイヤーは表現に「改良」を加えることを学習\n",
        "\n",
        "**Pre-LN vs Post-LN:**\n",
        "\n",
        "| 側面 | Pre-LN (GPT-2) | Post-LN (元のTransformer) |\n",
        "|--------|----------------|-------------------------------|\n",
        "| 式 | $x + \\text{SubLayer}(\\text{LN}(x))$ | $\\text{LN}(x + \\text{SubLayer}(x))$ |\n",
        "| 訓練 | より安定 | ウォームアップなしでは不安定になりうる |\n",
        "| 出力スケール | 深さとともに増加 | 各層で正規化 |\n",
        "\n",
        "**GPT-2のスタック:**\n",
        "- 124M: 12ブロック\n",
        "- 355M: 24ブロック\n",
        "- 774M: 36ブロック\n",
        "- 1558M: 48ブロック"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b5c2473",
      "metadata": {},
      "source": [
        "### GPT-2フォワードパス\n",
        "\n",
        "`gpt2()`関数はモデルの完全なフォワードパスです。トークンIDを入力として受け取り、各位置での次のトークンに対する語彙全体のロジット（正規化されていない確率）を出力します。\n",
        "\n",
        "**定義:**\n",
        "\n",
        "$$\\text{GPT-2}(\\mathbf{x}) = \\text{LayerNorm}(\\text{Blocks}(\\mathbf{E}_{token} + \\mathbf{E}_{pos})) \\cdot \\mathbf{W}_E^T$$\n",
        "\n",
        "**ステップバイステップ:**\n",
        "\n",
        "1. **トークン埋め込み**: 各入力トークンを埋め込み行列で検索\n",
        "   $$\\mathbf{h}_0^{(token)} = \\mathbf{W}_E[\\text{input\\_ids}] \\in \\mathbb{R}^{n_{seq} \\times d_{model}}$$\n",
        "\n",
        "2. **位置埋め込み**: 位置情報を追加\n",
        "   $$\\mathbf{h}_0 = \\mathbf{h}_0^{(token)} + \\mathbf{W}_P[0, 1, ..., n_{seq}-1]$$\n",
        "\n",
        "3. **Transformerブロック**: $L$個のTransformerブロックを通過\n",
        "   $$\\mathbf{h}_l = \\text{TransformerBlock}_l(\\mathbf{h}_{l-1}) \\quad \\text{for } l = 1, ..., L$$\n",
        "\n",
        "4. **最終レイヤー正規化**: 出力を正規化\n",
        "   $$\\mathbf{h}_{out} = \\text{LayerNorm}(\\mathbf{h}_L)$$\n",
        "\n",
        "5. **語彙への射影**: **共有**埋め込み行列を使用してロジットを計算\n",
        "   $$\\text{logits} = \\mathbf{h}_{out} \\cdot \\mathbf{W}_E^T \\in \\mathbb{R}^{n_{seq} \\times n_{vocab}}$$\n",
        "\n",
        "**アーキテクチャ概要:**\n",
        "```\n",
        "入力トークンID [n_seq]\n",
        "    ↓\n",
        "トークン埋め込み (wte): ルックアップテーブル [n_vocab, d_model]\n",
        "    +\n",
        "位置埋め込み (wpe): ルックアップテーブル [n_ctx, d_model]\n",
        "    ↓\n",
        "隠れ状態 [n_seq, d_model]\n",
        "    ↓\n",
        "┌─────────────────────────────────┐\n",
        "│   Transformerブロック 0          │\n",
        "│   (ln_1 → MHA → ln_2 → FFN)     │\n",
        "└─────────────────────────────────┘\n",
        "    ↓\n",
        "    ... (n_layer回繰り返し)\n",
        "    ↓\n",
        "┌─────────────────────────────────┐\n",
        "│   Transformerブロック L-1        │\n",
        "└─────────────────────────────────┘\n",
        "    ↓\n",
        "最終LayerNorm (ln_f)\n",
        "    ↓\n",
        "wte.Tとの行列積（重み共有）\n",
        "    ↓\n",
        "ロジット [n_seq, n_vocab]\n",
        "```\n",
        "\n",
        "**主要概念:**\n",
        "\n",
        "| 概念 | 説明 |\n",
        "|---------|-------------|\n",
        "| **トークン埋め込み** (`wte`) | 各トークンIDを密ベクトルにマッピングする学習済みルックアップテーブル |\n",
        "| **位置埋め込み** (`wpe`) | 位置情報（0からn_ctx-1）をエンコードする学習済みベクトル |\n",
        "| **重み共有** | 出力射影は別の重みの代わりに`wte.T`を再利用し、パラメータを削減 |\n",
        "| **最終LayerNorm** (`ln_f`) | すべてのブロックの後、語彙への射影前に適用 |\n",
        "\n",
        "**重み共有の洞察:**\n",
        "\n",
        "GPT-2は**重み共有**を使用 — 同じ埋め込み行列$\\mathbf{W}_E$が以下に使用されます：\n",
        "- **入力**: トークンID → ベクトル（行検索）\n",
        "- **出力**: ベクトル → ロジット（転置との行列積）\n",
        "\n",
        "これによりパラメータが削減され、類似トークンが類似のロジットを持つ意味のある出力空間が作成されます。\n",
        "\n",
        "**GPT-2モデルサイズ:**\n",
        "\n",
        "| モデル | $n_{layer}$ | $d_{model}$ | $n_{head}$ | $n_{ctx}$ | $n_{vocab}$ |\n",
        "|-------|-------------|-------------|------------|-----------|-------------|\n",
        "| 124M  | 12          | 768         | 12         | 1024      | 50257       |\n",
        "| 355M  | 24          | 1024        | 16         | 1024      | 50257       |\n",
        "| 774M  | 36          | 1280        | 20         | 1024      | 50257       |\n",
        "| 1558M | 48          | 1600        | 25         | 1024      | 50257       |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e33b08b1",
      "metadata": {
        "id": "e33b08b1"
      },
      "outputs": [],
      "source": [
        "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    \"\"\"A single transformer block.\"\"\"\n",
        "    # multi-head causal self attention\n",
        "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # position-wise feed forward network\n",
        "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
        "    \"\"\"GPT-2 forward pass.\"\"\"\n",
        "    # token + positional embeddings\n",
        "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
        "\n",
        "    # forward pass through n_layer transformer blocks\n",
        "    for block in blocks:\n",
        "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # projection to vocab\n",
        "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f1884f",
      "metadata": {
        "id": "22f1884f"
      },
      "source": [
        "## 5. テキスト生成関数"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4afea54c",
      "metadata": {},
      "source": [
        "### 自己回帰生成\n",
        "\n",
        "`generate()`関数は**自己回帰テキスト生成**を実装します — モデルは一度に1つのトークンを予測し、その予測を入力としてフィードバックして次のトークンを予測します。\n",
        "\n",
        "**定義:**\n",
        "\n",
        "入力トークン$\\mathbf{x} = [x_1, x_2, ..., x_n]$が与えられた場合、$T$個の新しいトークンを生成：\n",
        "\n",
        "$$x_{n+t} = \\arg\\max_{v \\in \\mathcal{V}} P(v \\mid x_1, ..., x_{n+t-1}) \\quad \\text{for } t = 1, ..., T$$\n",
        "\n",
        "ここで$\\mathcal{V}$は語彙であり、確率はロジットに対するソフトマックスから来ます。\n",
        "\n",
        "**ステップバイステップアルゴリズム:**\n",
        "\n",
        "```\n",
        "入力: プロンプトトークン [x₁, x₂, ..., xₙ], トークン数 T\n",
        "出力: 生成されたトークン [xₙ₊₁, xₙ₊₂, ..., xₙ₊ₜ]\n",
        "\n",
        "for t = 1 to T:\n",
        "    1. フォワードパス: logits = GPT2([x₁, ..., xₙ₊ₜ₋₁])\n",
        "    2. 最後の位置を取得: last_logits = logits[-1]  # [n_vocab]\n",
        "    3. 次のトークンを選択: xₙ₊ₜ = argmax(last_logits)\n",
        "    4. シーケンスに追加: [x₁, ..., xₙ₊ₜ₋₁] → [x₁, ..., xₙ₊ₜ]\n",
        "\n",
        "return [xₙ₊₁, ..., xₙ₊ₜ]\n",
        "```\n",
        "\n",
        "**視覚的表現:**\n",
        "\n",
        "```\n",
        "ステップ1: \"The cat\" → GPT-2 → logits → argmax → \"sat\"\n",
        "ステップ2: \"The cat sat\" → GPT-2 → logits → argmax → \"on\"\n",
        "ステップ3: \"The cat sat on\" → GPT-2 → logits → argmax → \"the\"\n",
        "...\n",
        "```\n",
        "\n",
        "**なぜ`logits[-1]`のみを使用するのか？**\n",
        "\n",
        "GPT-2はシーケンス内の**すべての位置**に対してロジットを出力します$[n_{seq}, n_{vocab}]$。しかし生成では、最後のトークンの**後**に来るものを予測することだけに関心があるため、`logits[-1]`を取ります。\n",
        "\n",
        "**デコーディング戦略:**\n",
        "\n",
        "| 戦略 | 式 | 特性 |\n",
        "|----------|---------|------------|\n",
        "| **貪欲法**（ここで使用） | $x = \\arg\\max(logits)$ | 決定論的、高速、繰り返しになりやすい |\n",
        "| **温度サンプリング** | $x \\sim \\text{softmax}(logits / \\tau)$ | $\\tau < 1$: より鋭く、$\\tau > 1$: よりランダム |\n",
        "| **Top-kサンプリング** | 上位$k$トークンからサンプル | 最も可能性の高い選択肢に制限 |\n",
        "| **Top-p（核）** | 累積$p$を持つ最小セットからサンプル | 適応的な語彙サイズ |\n",
        "\n",
        "**貪欲デコーディング:**\n",
        "\n",
        "この実装は**貪欲デコーディング**を使用 — 常に最も確率の高い次のトークンを選択：\n",
        "\n",
        "$$x_{next} = \\arg\\max_{v} \\text{logits}[v]$$\n",
        "\n",
        "利点：\n",
        "- シンプルで高速\n",
        "- 決定論的（同じ入力 → 同じ出力）\n",
        "\n",
        "欠点：\n",
        "- 繰り返しループに陥りやすい\n",
        "- より良いシーケンスを見逃す可能性（探索なし）\n",
        "- グローバル最適ではない（局所的な決定が最適でないテキストにつながる可能性）\n",
        "\n",
        "**計算上の注意:**\n",
        "\n",
        "この素朴な実装は各ステップでシーケンス全体を再計算します。実際には、**KVキャッシング**が中間アテンション状態を保存して冗長な計算を避け、トークンあたり$O(n^2)$ではなく$O(n)$で生成を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ace0a30d",
      "metadata": {
        "id": "ace0a30d"
      },
      "outputs": [],
      "source": [
        "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
        "    \"\"\"Generate tokens autoregressively using greedy decoding.\"\"\"\n",
        "    for _ in tqdm(range(n_tokens_to_generate), desc=\"Generating\"):\n",
        "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
        "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
        "        inputs.append(int(next_id))  # append prediction to input\n",
        "\n",
        "    return inputs[len(inputs) - n_tokens_to_generate:]  # only return generated ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6b81ef",
      "metadata": {
        "id": "6d6b81ef"
      },
      "source": [
        "## 6. モデルの重みとトークナイザーのロード\n",
        "\n",
        "事前学習済みGPT-2モデルをロードします。モデルが存在しない場合は自動的にダウンロードされます。\n",
        "\n",
        "利用可能なモデルサイズ：\n",
        "- `124M` - Small（デフォルト）\n",
        "- `355M` - Medium\n",
        "- `774M` - Large\n",
        "- `1558M` - XL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "19bb5772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19bb5772",
        "outputId": "43dbfc64-d9fe-40de-e5b9-018266a7433f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GPT-2 124M model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kb [00:00, 2.98Mb/s]                                                       \n",
            "Fetching encoder.json: 1.04Mb [00:00, 2.60Mb/s]                                                     \n",
            "Fetching hparams.json: 1.00kb [00:00, 4.52Mb/s]                                                     \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mb [00:30, 16.3Mb/s]                                    \n",
            "Fetching model.ckpt.index: 6.00kb [00:00, 7.34Mb/s]                                                 \n",
            "Fetching model.ckpt.meta: 472kb [00:00, 1.67Mb/s]                                                   \n",
            "Fetching vocab.bpe: 457kb [00:00, 1.56Mb/s]                                                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MODEL_SIZE = \"124M\"  # Choose from: \"124M\", \"355M\", \"774M\", \"1558M\"\n",
        "MODELS_DIR = \"models\"  # Directory to store downloaded models\n",
        "\n",
        "# Load encoder (tokenizer), hyperparameters, and model parameters\n",
        "print(f\"Loading GPT-2 {MODEL_SIZE} model...\")\n",
        "encoder, hparams, params = load_encoder_hparams_and_params(MODEL_SIZE, MODELS_DIR)\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "da02c04d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da02c04d",
        "outputId": "f624376f-2037-4a42-f412-7853257237a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Hyperparameters:\n",
            "  - Number of layers (n_layer): 12\n",
            "  - Number of attention heads (n_head): 12\n",
            "  - Embedding dimension (n_embd): 768\n",
            "  - Vocabulary size (n_vocab): 50257\n",
            "  - Context length (n_ctx): 1024\n"
          ]
        }
      ],
      "source": [
        "# Display model hyperparameters\n",
        "print(\"Model Hyperparameters:\")\n",
        "print(f\"  - Number of layers (n_layer): {hparams['n_layer']}\")\n",
        "print(f\"  - Number of attention heads (n_head): {hparams['n_head']}\")\n",
        "print(f\"  - Embedding dimension (n_embd): {hparams['n_embd']}\")\n",
        "print(f\"  - Vocabulary size (n_vocab): {hparams['n_vocab']}\")\n",
        "print(f\"  - Context length (n_ctx): {hparams['n_ctx']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "770f497c",
      "metadata": {
        "id": "770f497c"
      },
      "source": [
        "## 7. 推論の実行\n",
        "\n",
        "テキストを生成してみましょう！プロンプトと生成するトークン数を変更できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "13303e7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13303e7b",
        "outputId": "cbe95fbc-c546-40b7-a926-2033ab6c52f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Alan Turing theorized that computers would one day become\n",
            "Generating 40 tokens...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Input prompt\n",
        "prompt = \"Alan Turing theorized that computers would one day become\"\n",
        "\n",
        "# Number of tokens to generate\n",
        "n_tokens_to_generate = 40\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generating {n_tokens_to_generate} tokens...\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ec71058d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec71058d",
        "outputId": "4ef293c5-1e31-493f-a3a0-c04ba02c7dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input token IDs: [36235, 39141, 18765, 1143, 326, 9061, 561, 530, 1110, 1716]\n",
            "Number of input tokens: 10\n"
          ]
        }
      ],
      "source": [
        "# Encode the input prompt\n",
        "input_ids = encoder.encode(prompt)\n",
        "print(f\"Input token IDs: {input_ids}\")\n",
        "print(f\"Number of input tokens: {len(input_ids)}\")\n",
        "\n",
        "# Make sure we don't exceed the context length\n",
        "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"], \\\n",
        "    f\"Total tokens ({len(input_ids) + n_tokens_to_generate}) exceeds context length ({hparams['n_ctx']})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "43b9ef51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43b9ef51",
        "outputId": "b35dd109-017d-49b7-9a1f-d4d9eb3f573a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 40/40 [01:14<00:00,  1.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Generated Text:\n",
            "==================================================\n",
            "Alan Turing theorized that computers would one day become the most powerful machines on the planet.\n",
            "\n",
            "The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate output tokens\n",
        "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
        "\n",
        "# Decode the generated tokens back to text\n",
        "output_text = encoder.decode(output_ids)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Generated Text:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"{prompt}{output_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e6fca20",
      "metadata": {
        "id": "6e6fca20"
      },
      "source": [
        "## 8. インタラクティブ生成\n",
        "\n",
        "下で異なるプロンプトを試してみてください！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8aff5314",
      "metadata": {
        "id": "8aff5314"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, n_tokens=40):\n",
        "    \"\"\"Helper function to generate text from a prompt.\"\"\"\n",
        "    input_ids = encoder.encode(prompt)\n",
        "\n",
        "    if len(input_ids) + n_tokens >= hparams[\"n_ctx\"]:\n",
        "        print(f\"Warning: Reducing tokens to fit context length\")\n",
        "        n_tokens = hparams[\"n_ctx\"] - len(input_ids) - 1\n",
        "\n",
        "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens)\n",
        "    output_text = encoder.decode(output_ids)\n",
        "\n",
        "    return prompt + output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0c77da7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c77da7e",
        "outputId": "776ea070-34ec-407a-f405-3e3a36022489"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 50/50 [01:29<00:00,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The future of artificial intelligence is uncertain.\n",
            "\n",
            "\"We're not sure what the future will look like,\" said Dr. Michael S. Schoenfeld, a professor of computer science at the University of California, Berkeley. \"But we're not sure what the future will look\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Try your own prompts!\n",
        "my_prompt = \"The future of artificial intelligence is\"\n",
        "result = generate_text(my_prompt, n_tokens=50)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "86f3f930",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f3f930",
        "outputId": "0a59e19c-4934-4c21-97e5-5c59c389123e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 50/50 [01:27<00:00,  1.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In a world where robots are becoming more and more commonplace, it's important to remember that robots are not just a threat to humanity, but also to the planet.\n",
            "\n",
            "The robots that are currently in use are the ones that are currently being used to make the most of\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Another example\n",
        "my_prompt = \"In a world where robots\"\n",
        "result = generate_text(my_prompt, n_tokens=50)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e6a8d7",
      "metadata": {
        "id": "71e6a8d7"
      },
      "source": [
        "## 9. モデルアーキテクチャの理解\n",
        "\n",
        "モデルのパラメータを探索して、その構造をより深く理解しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ff163927",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff163927",
        "outputId": "0fa09c93-d1ce-4a5d-8b1c-73d7f71fc788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-level parameters:\n",
            "  ln_f: ['b', 'g']\n",
            "  wpe: shape = (1024, 768)\n",
            "  wte: shape = (50257, 768)\n",
            "\n",
            "Number of transformer blocks: 12\n"
          ]
        }
      ],
      "source": [
        "# Explore the parameter structure\n",
        "print(\"Top-level parameters:\")\n",
        "for key in params.keys():\n",
        "    if key != 'blocks':\n",
        "        if isinstance(params[key], dict):\n",
        "            print(f\"  {key}: {list(params[key].keys())}\")\n",
        "        else:\n",
        "            print(f\"  {key}: shape = {params[key].shape}\")\n",
        "\n",
        "print(f\"\\nNumber of transformer blocks: {len(params['blocks'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "193f1b95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "193f1b95",
        "outputId": "4ab098bb-77b5-40bd-b3c3-d11dca46045e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structure of a transformer block:\n",
            "  attn:\n",
            "    c_attn:\n",
            "      b: shape = (2304,)\n",
            "      w: shape = (768, 2304)\n",
            "    c_proj:\n",
            "      b: shape = (768,)\n",
            "      w: shape = (768, 768)\n",
            "  ln_1:\n",
            "    b: shape = (768,)\n",
            "    g: shape = (768,)\n",
            "  ln_2:\n",
            "    b: shape = (768,)\n",
            "    g: shape = (768,)\n",
            "  mlp:\n",
            "    c_fc:\n",
            "      b: shape = (3072,)\n",
            "      w: shape = (768, 3072)\n",
            "    c_proj:\n",
            "      b: shape = (768,)\n",
            "      w: shape = (3072, 768)\n"
          ]
        }
      ],
      "source": [
        "# Explore a single transformer block\n",
        "block = params['blocks'][0]\n",
        "print(\"Structure of a transformer block:\")\n",
        "for key, value in block.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"  {key}:\")\n",
        "        for k, v in value.items():\n",
        "            # Check if v is also a dictionary, if so, iterate further\n",
        "            if isinstance(v, dict):\n",
        "                print(f\"    {k}:\")\n",
        "                for sub_k, sub_v in v.items():\n",
        "                    if isinstance(sub_v, np.ndarray):\n",
        "                        print(f\"      {sub_k}: shape = {sub_v.shape}\")\n",
        "                    else:\n",
        "                        print(f\"      {sub_k}: {type(sub_v)}\")\n",
        "            elif isinstance(v, np.ndarray): # if v is a numpy array directly\n",
        "                print(f\"    {k}: shape = {v.shape}\")\n",
        "            else: # Fallback for unexpected types for v\n",
        "                print(f\"    {k}: {type(v)}\")\n",
        "    elif isinstance(value, np.ndarray): # if value is a numpy array directly\n",
        "        print(f\"  {key}: shape = {value.shape}\")\n",
        "    else: # Fallback for unexpected types for value\n",
        "        print(f\"  {key}: {type(value)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ad53ac2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad53ac2a",
        "outputId": "8942d0c1-bd5e-4366-9a1d-ea760493fa05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 124,439,808\n",
            "Approximately: 124.4M parameters\n"
          ]
        }
      ],
      "source": [
        "# Calculate total number of parameters\n",
        "def count_params(d):\n",
        "    total = 0\n",
        "    for key, value in d.items():\n",
        "        if isinstance(value, dict):\n",
        "            total += count_params(value)\n",
        "        elif isinstance(value, list):\n",
        "            for item in value:\n",
        "                if isinstance(item, dict):\n",
        "                    total += count_params(item)\n",
        "        elif isinstance(value, np.ndarray):\n",
        "            total += value.size\n",
        "    return total\n",
        "\n",
        "total_params = count_params(params)\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "print(f\"Approximately: {total_params / 1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48033bdd",
      "metadata": {
        "id": "48033bdd"
      },
      "source": [
        "## 10. トークナイザーの探索\n",
        "\n",
        "BPEトークナイザーがどのように動作するか見てみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "491c13c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "491c13c5",
        "outputId": "89263b61-a66e-4146-e5b4-20b4b077f461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: 'Hello, world!'\n",
            "Tokens: [15496, 11, 995, 0]\n",
            "Decoded: 'Hello, world!'\n",
            "Number of tokens: 4\n",
            "\n",
            "Original: 'GPT-2 is a large language model.'\n",
            "Tokens: [38, 11571, 12, 17, 318, 257, 1588, 3303, 2746, 13]\n",
            "Decoded: 'GPT-2 is a large language model.'\n",
            "Number of tokens: 10\n",
            "\n",
            "Original: 'The quick brown fox jumps over the lazy dog.'\n",
            "Tokens: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
            "Decoded: 'The quick brown fox jumps over the lazy dog.'\n",
            "Number of tokens: 10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Encode and decode examples\n",
        "test_texts = [\n",
        "    \"Hello, world!\",\n",
        "    \"GPT-2 is a large language model.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = encoder.encode(text)\n",
        "    decoded = encoder.decode(tokens)\n",
        "    print(f\"Original: '{text}'\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Decoded: '{decoded}'\")\n",
        "    print(f\"Number of tokens: {len(tokens)}\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
