{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70ff0c66",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-0/picoGPT_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a12e2da",
      "metadata": {
        "id": "8a12e2da"
      },
      "source": [
        "# PicoGPT Inference Notebook\n",
        "\n",
        "This notebook demonstrates how to load and run inference with GPT-2 using the picoGPT implementation.\n",
        "\n",
        "picoGPT is a minimal implementation of GPT-2 in pure NumPy, making it easy to understand the core concepts of transformer-based language models.\n",
        "\n",
        "**Features:**\n",
        "- Load pre-trained GPT-2 weights\n",
        "- BPE tokenization\n",
        "- Text generation with greedy decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5f4db3",
      "metadata": {
        "id": "ed5f4db3"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c42a6bd1",
      "metadata": {
        "id": "c42a6bd1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from functools import lru_cache\n",
        "\n",
        "import numpy as np\n",
        "import regex\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b6cf84",
      "metadata": {
        "id": "d1b6cf84"
      },
      "source": [
        "## 2. BPE Tokenizer (Encoder)\n",
        "\n",
        "The GPT-2 tokenizer uses Byte Pair Encoding (BPE) to convert text into tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "60f98bd1",
      "metadata": {
        "id": "60f98bd1"
      },
      "outputs": [],
      "source": [
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8 + n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    \"\"\"BPE Encoder/Decoder for GPT-2.\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.errors = errors\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "        self.pat = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = \" \".join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encode text to token IDs.\"\"\"\n",
        "        bpe_tokens = []\n",
        "        for token in regex.findall(self.pat, text):\n",
        "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"Decode token IDs back to text.\"\"\"\n",
        "        text = \"\".join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
        "        return text\n",
        "\n",
        "\n",
        "def get_encoder(model_name, models_dir):\n",
        "    \"\"\"Load the BPE encoder from files.\"\"\"\n",
        "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
        "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40db22d5",
      "metadata": {
        "id": "40db22d5"
      },
      "source": [
        "## 3. Model Loading Utilities\n",
        "\n",
        "Functions to download GPT-2 weights from OpenAI and load them from TensorFlow checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f2187272",
      "metadata": {
        "id": "f2187272"
      },
      "outputs": [],
      "source": [
        "def download_gpt2_files(model_size, model_dir):\n",
        "    \"\"\"Download GPT-2 model files from OpenAI.\"\"\"\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "    for filename in [\n",
        "        \"checkpoint\",\n",
        "        \"encoder.json\",\n",
        "        \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\",\n",
        "        \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\",\n",
        "        \"vocab.bpe\",\n",
        "    ]:\n",
        "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
        "        r.raise_for_status()\n",
        "\n",
        "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
        "            file_size = int(r.headers[\"content-length\"])\n",
        "            chunk_size = 1000\n",
        "            with tqdm(\n",
        "                ncols=100,\n",
        "                desc=\"Fetching \" + filename,\n",
        "                total=file_size,\n",
        "                unit_scale=True,\n",
        "                unit=\"b\",\n",
        "            ) as pbar:\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(chunk_size)\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n",
        "    \"\"\"Load GPT-2 parameters from TensorFlow checkpoint.\"\"\"\n",
        "    def set_in_nested_dict(d, keys, val):\n",
        "        if not keys:\n",
        "            return val\n",
        "        if keys[0] not in d:\n",
        "            d[keys[0]] = {}\n",
        "        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n",
        "        return d\n",
        "\n",
        "    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n",
        "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
        "        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
        "        name = name[len(\"model/\") :]\n",
        "        if name.startswith(\"h\"):\n",
        "            m = re.match(r\"h([0-9]+)/(.*)\", name)\n",
        "            n = int(m[1])\n",
        "            sub_name = m[2]\n",
        "            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n",
        "        else:\n",
        "            set_in_nested_dict(params, name.split(\"/\"), array)\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def load_encoder_hparams_and_params(model_size, models_dir):\n",
        "    \"\"\"Load encoder, hyperparameters, and model parameters.\"\"\"\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    if not tf_ckpt_path:  # download files if necessary\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        download_gpt2_files(model_size, model_dir)\n",
        "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "\n",
        "    encoder = get_encoder(model_size, models_dir)\n",
        "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
        "\n",
        "    return encoder, hparams, params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f84949bb",
      "metadata": {
        "id": "f84949bb"
      },
      "source": [
        "## 4. GPT-2 Model Components\n",
        "\n",
        "These are the core building blocks of the GPT-2 architecture, implemented in pure NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9920bff",
      "metadata": {},
      "source": [
        "### GELU (Gaussian Error Linear Unit)\n",
        "\n",
        "GELU is the activation function used in GPT-2 (and many modern transformers like BERT, GPT-3, etc.). Unlike ReLU which has a hard cutoff at 0, GELU provides a smooth, probabilistic gating mechanism.\n",
        "\n",
        "**Exact Definition:**\n",
        "\n",
        "$$\\text{GELU}(x) = x \\cdot \\Phi(x) = x \\cdot P(X \\leq x)$$\n",
        "\n",
        "where $\\Phi(x)$ is the cumulative distribution function (CDF) of the standard normal distribution.\n",
        "\n",
        "**Approximation (used in practice):**\n",
        "\n",
        "Since computing the exact CDF is expensive, GPT-2 uses a fast approximation based on $\\tanh$:\n",
        "\n",
        "$$\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)\\right)\\right)$$\n",
        "\n",
        "**Why GELU over ReLU?**\n",
        "- **Smooth**: Differentiable everywhere (no kink at 0)\n",
        "- **Non-monotonic**: Small negative values can have small positive outputs\n",
        "- **Stochastic interpretation**: Can be seen as multiplying input by a Bernoulli mask where the probability depends on the input value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7474f4e1",
      "metadata": {},
      "source": [
        "### Softmax\n",
        "\n",
        "Softmax converts a vector of real numbers (logits) into a probability distribution. It's used in attention mechanisms and the final output layer.\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "For a vector $\\mathbf{x} = [x_1, x_2, ..., x_n]$:\n",
        "\n",
        "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
        "\n",
        "**Properties:**\n",
        "- Output values are in range $(0, 1)$\n",
        "- All outputs sum to 1: $\\sum_i \\text{softmax}(x_i) = 1$\n",
        "- Preserves ordering: if $x_i > x_j$, then $\\text{softmax}(x_i) > \\text{softmax}(x_j)$\n",
        "\n",
        "**Numerical Stability Issue:**\n",
        "\n",
        "Computing $e^{x_i}$ directly can cause overflow for large $x$. The solution is to subtract the maximum value:\n",
        "\n",
        "$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(\\mathbf{x})}}{\\sum_{j=1}^{n} e^{x_j - \\max(\\mathbf{x})}}$$\n",
        "\n",
        "This is mathematically equivalent (the $e^{-\\max(\\mathbf{x})}$ cancels out) but prevents overflow since the largest exponent is now $e^0 = 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a163d9b",
      "metadata": {},
      "source": [
        "### Layer Normalization\n",
        "\n",
        "Layer normalization stabilizes training by normalizing activations across the feature dimension. Unlike batch normalization (which normalizes across the batch), layer norm works on individual samples.\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "For an input vector $\\mathbf{x} = [x_1, x_2, ..., x_d]$ of dimension $d$:\n",
        "\n",
        "$$\\text{LayerNorm}(\\mathbf{x}) = \\gamma \\cdot \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
        "\n",
        "where:\n",
        "- $\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i$ is the mean\n",
        "- $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2$ is the variance\n",
        "- $\\gamma$ (scale) and $\\beta$ (shift) are learnable parameters\n",
        "- $\\epsilon$ is a small constant (e.g., $10^{-5}$) for numerical stability\n",
        "\n",
        "**Why Layer Norm?**\n",
        "- **Stabilizes gradients**: Prevents activations from becoming too large or too small\n",
        "- **Enables deeper networks**: Essential for training transformers with many layers\n",
        "- **Independent of batch size**: Works the same during training and inference\n",
        "\n",
        "**In GPT-2:**\n",
        "- Applied **before** attention and FFN (Pre-LN architecture)\n",
        "- Each transformer block has two layer norms: `ln_1` (before attention) and `ln_2` (before FFN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4609d1c3",
      "metadata": {},
      "source": [
        "### Linear (Fully Connected Layer)\n",
        "\n",
        "The linear transformation is the fundamental building block of neural networks. It's also called a fully connected layer, dense layer, or affine transformation.\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "For input $\\mathbf{x} \\in \\mathbb{R}^{d_{in}}$, weight matrix $\\mathbf{W} \\in \\mathbb{R}^{d_{in} \\times d_{out}}$, and bias $\\mathbf{b} \\in \\mathbb{R}^{d_{out}}$:\n",
        "\n",
        "$$\\text{Linear}(\\mathbf{x}) = \\mathbf{x} \\mathbf{W} + \\mathbf{b}$$\n",
        "\n",
        "**Shape transformation:**\n",
        "- Input: $[m, d_{in}]$ (batch of $m$ vectors)\n",
        "- Weight: $[d_{in}, d_{out}]$\n",
        "- Bias: $[d_{out}]$\n",
        "- Output: $[m, d_{out}]$\n",
        "\n",
        "**In GPT-2, linear layers are used for:**\n",
        "- **QKV projection** (`c_attn`): Projects input to query, key, value vectors\n",
        "- **Output projection** (`c_proj`): Projects attention output back to embedding dimension\n",
        "- **FFN up-projection** (`c_fc`): Expands from $d_{model}$ to $4 \\cdot d_{model}$\n",
        "- **FFN down-projection** (`c_proj`): Compresses back to $d_{model}$\n",
        "\n",
        "**Note:** The `@` operator in NumPy/Python performs matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "540fdfa5",
      "metadata": {
        "id": "540fdfa5"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    \"\"\"Gaussian Error Linear Unit activation function.\"\"\"\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Numerically stable softmax function.\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "\n",
        "def layer_norm(x, g, b, eps: float = 1e-5):\n",
        "    \"\"\"Layer normalization.\"\"\"\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
        "    return g * x + b  # scale and offset with gamma/beta params\n",
        "\n",
        "\n",
        "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
        "    \"\"\"Linear transformation.\"\"\"\n",
        "    return x @ w + b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "768eef82",
      "metadata": {},
      "source": [
        "### Feed-Forward Network (FFN / MLP)\n",
        "\n",
        "The Feed-Forward Network (also called MLP or position-wise feed-forward) is applied independently to each position in the sequence. It consists of two linear transformations with a GELU activation in between.\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "$$\\text{FFN}(\\mathbf{x}) = \\text{Linear}_2(\\text{GELU}(\\text{Linear}_1(\\mathbf{x})))$$\n",
        "\n",
        "Or more explicitly:\n",
        "\n",
        "$$\\text{FFN}(\\mathbf{x}) = \\text{GELU}(\\mathbf{x} \\mathbf{W}_1 + \\mathbf{b}_1) \\mathbf{W}_2 + \\mathbf{b}_2$$\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Input [n_seq, d_model]\n",
        "    ↓\n",
        "Linear (up-projection): d_model → 4 × d_model\n",
        "    ↓\n",
        "GELU activation\n",
        "    ↓\n",
        "Linear (down-projection): 4 × d_model → d_model\n",
        "    ↓\n",
        "Output [n_seq, d_model]\n",
        "```\n",
        "\n",
        "**Why 4× expansion?**\n",
        "- The intermediate dimension is typically $4 \\times d_{model}$ (e.g., 768 → 3072 for GPT-2 small)\n",
        "- This \"bottleneck\" design allows the network to learn richer representations\n",
        "- The expansion provides more capacity for non-linear transformations\n",
        "\n",
        "**In GPT-2:**\n",
        "- `c_fc`: The up-projection weights (expands dimension)\n",
        "- `c_proj`: The down-projection weights (compresses back)\n",
        "- Applied after layer norm in each transformer block"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a78d33",
      "metadata": {},
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "Attention is the core mechanism that allows the model to focus on relevant parts of the input when producing each output. It computes a weighted sum of values, where the weights are determined by the similarity between queries and keys.\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\text{mask}\\right) V$$\n",
        "\n",
        "where:\n",
        "- $Q \\in \\mathbb{R}^{n_q \\times d_k}$ — Query matrix (what we're looking for)\n",
        "- $K \\in \\mathbb{R}^{n_k \\times d_k}$ — Key matrix (what we match against)\n",
        "- $V \\in \\mathbb{R}^{n_k \\times d_v}$ — Value matrix (what we retrieve)\n",
        "- $d_k$ — Dimension of keys (used for scaling)\n",
        "- mask — Causal mask to prevent attending to future tokens\n",
        "\n",
        "**Step-by-step breakdown:**\n",
        "\n",
        "1. **Compute attention scores**: $QK^T$ gives similarity between each query and key\n",
        "2. **Scale**: Divide by $\\sqrt{d_k}$ to prevent softmax saturation for large $d_k$\n",
        "3. **Mask**: Add $-\\infty$ (or $-10^{10}$) to positions we shouldn't attend to\n",
        "4. **Softmax**: Convert scores to probabilities (rows sum to 1)\n",
        "5. **Weighted sum**: Multiply by $V$ to get the output\n",
        "\n",
        "**Why scale by $\\sqrt{d_k}$?**\n",
        "\n",
        "Without scaling, when $d_k$ is large, the dot products $q \\cdot k$ grow large in magnitude, pushing softmax into regions with extremely small gradients. Scaling by $\\sqrt{d_k}$ keeps the variance stable.\n",
        "\n",
        "**Causal Mask (for GPT):**\n",
        "\n",
        "For autoregressive models, we use a lower-triangular mask so position $i$ can only attend to positions $\\leq i$:\n",
        "\n",
        "$$\\text{mask}_{ij} = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e00f7fd4",
      "metadata": {},
      "source": [
        "### Multi-Head Attention (MHA)\n",
        "\n",
        "Multi-Head Attention runs multiple attention operations in parallel, each with different learned projections. This allows the model to jointly attend to information from different representation subspaces.\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "$$\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O$$\n",
        "\n",
        "where each head is:\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)$$\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Input X [n_seq, d_model]\n",
        "    ↓\n",
        "Linear projection → Q, K, V [n_seq, 3 × d_model]\n",
        "    ↓\n",
        "Split into n_head heads → each head has dim d_k = d_model / n_head\n",
        "    ↓\n",
        "Parallel attention on each head (with causal mask)\n",
        "    ↓\n",
        "Concatenate heads → [n_seq, d_model]\n",
        "    ↓\n",
        "Output projection → [n_seq, d_model]\n",
        "```\n",
        "\n",
        "**Why Multiple Heads?**\n",
        "\n",
        "- **Different attention patterns**: Each head can learn to focus on different aspects (e.g., syntax vs. semantics, nearby vs. distant tokens)\n",
        "- **Richer representations**: More expressive than a single attention with the same total parameters\n",
        "- **Parallel computation**: All heads compute independently, enabling efficient parallelization\n",
        "\n",
        "**GPT-2 Configuration:**\n",
        "| Model | $d_{model}$ | $n_{head}$ | $d_k = d_{model}/n_{head}$ |\n",
        "|-------|-------------|------------|----------------------------|\n",
        "| 124M  | 768         | 12         | 64                         |\n",
        "| 355M  | 1024        | 16         | 64                         |\n",
        "| 774M  | 1280        | 20         | 64                         |\n",
        "| 1558M | 1600        | 25         | 64                         |\n",
        "\n",
        "**In the code:**\n",
        "- `c_attn`: Combined QKV projection weights $[d_{model}, 3 \\times d_{model}]$\n",
        "- `c_proj`: Output projection weights $[d_{model}, d_{model}]$\n",
        "- `np.split(..., 3)`: Splits the projection into Q, K, V\n",
        "- `np.split(..., n_head)`: Splits each into multiple heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7fae8700",
      "metadata": {
        "id": "7fae8700"
      },
      "outputs": [],
      "source": [
        "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    \"\"\"Feed-forward network (MLP) in transformer.\"\"\"\n",
        "    # project up\n",
        "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
        "\n",
        "    # project back down\n",
        "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
        "    \"\"\"Scaled dot-product attention.\"\"\"\n",
        "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
        "\n",
        "\n",
        "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    \"\"\"Multi-head attention.\"\"\"\n",
        "    # qkv projection\n",
        "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
        "\n",
        "    # split into heads\n",
        "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # causal mask to hide future inputs from being attended to\n",
        "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
        "\n",
        "    # perform attention over each head\n",
        "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # merge heads\n",
        "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
        "\n",
        "    # out projection\n",
        "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f27f058",
      "metadata": {},
      "source": [
        "### Transformer Block\n",
        "\n",
        "The transformer block is the fundamental repeating unit of GPT-2. Each block combines self-attention (for mixing information across positions) with a feed-forward network (for processing each position independently), connected via residual connections and layer normalization.\n",
        "\n",
        "**Definition (Pre-LN Architecture):**\n",
        "\n",
        "GPT-2 uses the **Pre-LayerNorm** variant, where normalization is applied *before* each sub-layer:\n",
        "\n",
        "$$\\mathbf{x} = \\mathbf{x} + \\text{MHA}(\\text{LayerNorm}(\\mathbf{x}))$$\n",
        "$$\\mathbf{x} = \\mathbf{x} + \\text{FFN}(\\text{LayerNorm}(\\mathbf{x}))$$\n",
        "\n",
        "**Architecture Diagram:**\n",
        "```\n",
        "Input x [n_seq, d_model]\n",
        "    │\n",
        "    ├───────────────────────────┐\n",
        "    ↓                           │ (residual)\n",
        "LayerNorm (ln_1)                │\n",
        "    ↓                           │\n",
        "Multi-Head Attention            │\n",
        "    ↓                           │\n",
        "    + ←─────────────────────────┘\n",
        "    │\n",
        "    ├───────────────────────────┐\n",
        "    ↓                           │ (residual)\n",
        "LayerNorm (ln_2)                │\n",
        "    ↓                           │\n",
        "Feed-Forward Network            │\n",
        "    ↓                           │\n",
        "    + ←─────────────────────────┘\n",
        "    ↓\n",
        "Output [n_seq, d_model]\n",
        "```\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "| Component | Purpose | Parameters |\n",
        "|-----------|---------|------------|\n",
        "| `ln_1` | Normalize before attention | $\\gamma_1, \\beta_1$ |\n",
        "| `attn` | Multi-head self-attention | $W^Q, W^K, W^V, W^O$ |\n",
        "| `ln_2` | Normalize before FFN | $\\gamma_2, \\beta_2$ |\n",
        "| `mlp` | Position-wise feed-forward | $W_1, b_1, W_2, b_2$ |\n",
        "\n",
        "**Why Residual Connections?**\n",
        "\n",
        "The `+` operations are **residual (skip) connections**, introduced in ResNet:\n",
        "\n",
        "$$\\text{output} = \\text{input} + F(\\text{input})$$\n",
        "\n",
        "Benefits:\n",
        "- **Gradient flow**: Gradients can flow directly through the skip connection, preventing vanishing gradients in deep networks\n",
        "- **Identity mapping**: If $F$ outputs zeros, the layer becomes an identity function, making optimization easier\n",
        "- **Incremental learning**: Each sub-layer learns to add \"refinements\" to the representation\n",
        "\n",
        "**Pre-LN vs Post-LN:**\n",
        "\n",
        "| Aspect | Pre-LN (GPT-2) | Post-LN (Original Transformer) |\n",
        "|--------|----------------|-------------------------------|\n",
        "| Formula | $x + \\text{SubLayer}(\\text{LN}(x))$ | $\\text{LN}(x + \\text{SubLayer}(x))$ |\n",
        "| Training | More stable | Can be unstable without warmup |\n",
        "| Output scale | Grows with depth | Normalized at each layer |\n",
        "\n",
        "**GPT-2 Stack:**\n",
        "- 124M: 12 blocks\n",
        "- 355M: 24 blocks  \n",
        "- 774M: 36 blocks\n",
        "- 1558M: 48 blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b5c2473",
      "metadata": {},
      "source": [
        "### GPT-2 Forward Pass\n",
        "\n",
        "The `gpt2()` function is the complete forward pass of the model. It takes token IDs as input and outputs logits (unnormalized probabilities) over the vocabulary for the next token at each position.\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "$$\\text{GPT-2}(\\mathbf{x}) = \\text{LayerNorm}(\\text{Blocks}(\\mathbf{E}_{token} + \\mathbf{E}_{pos})) \\cdot \\mathbf{W}_E^T$$\n",
        "\n",
        "**Step-by-step:**\n",
        "\n",
        "1. **Token Embedding**: Look up each input token in the embedding matrix\n",
        "   $$\\mathbf{h}_0^{(token)} = \\mathbf{W}_E[\\text{input\\_ids}] \\in \\mathbb{R}^{n_{seq} \\times d_{model}}$$\n",
        "\n",
        "2. **Positional Embedding**: Add position information\n",
        "   $$\\mathbf{h}_0 = \\mathbf{h}_0^{(token)} + \\mathbf{W}_P[0, 1, ..., n_{seq}-1]$$\n",
        "\n",
        "3. **Transformer Blocks**: Pass through $L$ transformer blocks\n",
        "   $$\\mathbf{h}_l = \\text{TransformerBlock}_l(\\mathbf{h}_{l-1}) \\quad \\text{for } l = 1, ..., L$$\n",
        "\n",
        "4. **Final Layer Norm**: Normalize the output\n",
        "   $$\\mathbf{h}_{out} = \\text{LayerNorm}(\\mathbf{h}_L)$$\n",
        "\n",
        "5. **Project to Vocabulary**: Compute logits using the **tied** embedding matrix\n",
        "   $$\\text{logits} = \\mathbf{h}_{out} \\cdot \\mathbf{W}_E^T \\in \\mathbb{R}^{n_{seq} \\times n_{vocab}}$$\n",
        "\n",
        "**Architecture Overview:**\n",
        "```\n",
        "Input token IDs [n_seq]\n",
        "    ↓\n",
        "Token Embedding (wte): lookup table [n_vocab, d_model]\n",
        "    +\n",
        "Position Embedding (wpe): lookup table [n_ctx, d_model]\n",
        "    ↓\n",
        "Hidden states [n_seq, d_model]\n",
        "    ↓\n",
        "┌─────────────────────────────────┐\n",
        "│   Transformer Block 0           │\n",
        "│   (ln_1 → MHA → ln_2 → FFN)     │\n",
        "└─────────────────────────────────┘\n",
        "    ↓\n",
        "    ... (repeat n_layer times)\n",
        "    ↓\n",
        "┌─────────────────────────────────┐\n",
        "│   Transformer Block L-1         │\n",
        "└─────────────────────────────────┘\n",
        "    ↓\n",
        "Final LayerNorm (ln_f)\n",
        "    ↓\n",
        "Matmul with wte.T (weight tying)\n",
        "    ↓\n",
        "Logits [n_seq, n_vocab]\n",
        "```\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "| Concept | Explanation |\n",
        "|---------|-------------|\n",
        "| **Token Embedding** (`wte`) | Learned lookup table mapping each token ID to a dense vector |\n",
        "| **Positional Embedding** (`wpe`) | Learned vectors encoding position information (0 to n_ctx-1) |\n",
        "| **Weight Tying** | Output projection reuses `wte.T` instead of separate weights, reducing parameters |\n",
        "| **Final LayerNorm** (`ln_f`) | Applied after all blocks, before projecting to vocabulary |\n",
        "\n",
        "**Weight Tying Insight:**\n",
        "\n",
        "GPT-2 uses **weight tying** — the same embedding matrix $\\mathbf{W}_E$ is used for:\n",
        "- **Input**: Converting token IDs → vectors (row lookup)\n",
        "- **Output**: Converting vectors → logits (matrix multiply with transpose)\n",
        "\n",
        "This reduces parameters and creates a meaningful output space where similar tokens have similar logits.\n",
        "\n",
        "**GPT-2 Model Sizes:**\n",
        "\n",
        "| Model | $n_{layer}$ | $d_{model}$ | $n_{head}$ | $n_{ctx}$ | $n_{vocab}$ |\n",
        "|-------|-------------|-------------|------------|-----------|-------------|\n",
        "| 124M  | 12          | 768         | 12         | 1024      | 50257       |\n",
        "| 355M  | 24          | 1024        | 16         | 1024      | 50257       |\n",
        "| 774M  | 36          | 1280        | 20         | 1024      | 50257       |\n",
        "| 1558M | 48          | 1600        | 25         | 1024      | 50257       |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e33b08b1",
      "metadata": {
        "id": "e33b08b1"
      },
      "outputs": [],
      "source": [
        "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    \"\"\"A single transformer block.\"\"\"\n",
        "    # multi-head causal self attention\n",
        "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # position-wise feed forward network\n",
        "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
        "    \"\"\"GPT-2 forward pass.\"\"\"\n",
        "    # token + positional embeddings\n",
        "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
        "\n",
        "    # forward pass through n_layer transformer blocks\n",
        "    for block in blocks:\n",
        "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # projection to vocab\n",
        "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f1884f",
      "metadata": {
        "id": "22f1884f"
      },
      "source": [
        "## 5. Text Generation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4afea54c",
      "metadata": {},
      "source": [
        "### Autoregressive Generation\n",
        "\n",
        "The `generate()` function implements **autoregressive text generation** — the model predicts one token at a time, then feeds that prediction back as input to predict the next token.\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "Given input tokens $\\mathbf{x} = [x_1, x_2, ..., x_n]$, generate $T$ new tokens:\n",
        "\n",
        "$$x_{n+t} = \\arg\\max_{v \\in \\mathcal{V}} P(v \\mid x_1, ..., x_{n+t-1}) \\quad \\text{for } t = 1, ..., T$$\n",
        "\n",
        "where $\\mathcal{V}$ is the vocabulary and the probability comes from softmax over logits.\n",
        "\n",
        "**Step-by-step Algorithm:**\n",
        "\n",
        "```\n",
        "Input: prompt tokens [x₁, x₂, ..., xₙ], number of tokens T\n",
        "Output: generated tokens [xₙ₊₁, xₙ₊₂, ..., xₙ₊ₜ]\n",
        "\n",
        "for t = 1 to T:\n",
        "    1. Forward pass: logits = GPT2([x₁, ..., xₙ₊ₜ₋₁])\n",
        "    2. Get last position: last_logits = logits[-1]  # [n_vocab]\n",
        "    3. Select next token: xₙ₊ₜ = argmax(last_logits)\n",
        "    4. Append to sequence: [x₁, ..., xₙ₊ₜ₋₁] → [x₁, ..., xₙ₊ₜ]\n",
        "\n",
        "return [xₙ₊₁, ..., xₙ₊ₜ]\n",
        "```\n",
        "\n",
        "**Visual Representation:**\n",
        "\n",
        "```\n",
        "Step 1: \"The cat\" → GPT-2 → logits → argmax → \"sat\"\n",
        "Step 2: \"The cat sat\" → GPT-2 → logits → argmax → \"on\"\n",
        "Step 3: \"The cat sat on\" → GPT-2 → logits → argmax → \"the\"\n",
        "...\n",
        "```\n",
        "\n",
        "**Why Only Use `logits[-1]`?**\n",
        "\n",
        "GPT-2 outputs logits for **every position** in the sequence $[n_{seq}, n_{vocab}]$. But for generation, we only care about predicting what comes **after** the last token, so we take `logits[-1]`.\n",
        "\n",
        "**Decoding Strategies:**\n",
        "\n",
        "| Strategy | Formula | Properties |\n",
        "|----------|---------|------------|\n",
        "| **Greedy** (used here) | $x = \\arg\\max(logits)$ | Deterministic, fast, can be repetitive |\n",
        "| **Temperature sampling** | $x \\sim \\text{softmax}(logits / \\tau)$ | $\\tau < 1$: sharper, $\\tau > 1$: more random |\n",
        "| **Top-k sampling** | Sample from top $k$ tokens | Limits to most likely options |\n",
        "| **Top-p (nucleus)** | Sample from smallest set with cumulative $p$ | Adaptive vocabulary size |\n",
        "\n",
        "**Greedy Decoding:**\n",
        "\n",
        "This implementation uses **greedy decoding** — always selecting the most probable next token:\n",
        "\n",
        "$$x_{next} = \\arg\\max_{v} \\text{logits}[v]$$\n",
        "\n",
        "Pros:\n",
        "- Simple and fast\n",
        "- Deterministic (same input → same output)\n",
        "\n",
        "Cons:\n",
        "- Can get stuck in repetitive loops\n",
        "- Misses potentially better sequences (no exploration)\n",
        "- Not globally optimal (local decisions can lead to suboptimal text)\n",
        "\n",
        "**Computational Note:**\n",
        "\n",
        "This naive implementation recomputes the entire sequence at each step. In practice, **KV caching** stores intermediate attention states to avoid redundant computation, making generation $O(n)$ instead of $O(n^2)$ per token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ace0a30d",
      "metadata": {
        "id": "ace0a30d"
      },
      "outputs": [],
      "source": [
        "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
        "    \"\"\"Generate tokens autoregressively using greedy decoding.\"\"\"\n",
        "    for _ in tqdm(range(n_tokens_to_generate), desc=\"Generating\"):\n",
        "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
        "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
        "        inputs.append(int(next_id))  # append prediction to input\n",
        "\n",
        "    return inputs[len(inputs) - n_tokens_to_generate:]  # only return generated ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6b81ef",
      "metadata": {
        "id": "6d6b81ef"
      },
      "source": [
        "## 6. Load Model Weights and Tokenizer\n",
        "\n",
        "We'll load the pre-trained GPT-2 model. The model will be downloaded automatically if not present.\n",
        "\n",
        "Available model sizes:\n",
        "- `124M` - Small (default)\n",
        "- `355M` - Medium\n",
        "- `774M` - Large\n",
        "- `1558M` - XL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "19bb5772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19bb5772",
        "outputId": "43dbfc64-d9fe-40de-e5b9-018266a7433f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GPT-2 124M model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kb [00:00, 2.98Mb/s]                                                       \n",
            "Fetching encoder.json: 1.04Mb [00:00, 2.60Mb/s]                                                     \n",
            "Fetching hparams.json: 1.00kb [00:00, 4.52Mb/s]                                                     \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mb [00:30, 16.3Mb/s]                                    \n",
            "Fetching model.ckpt.index: 6.00kb [00:00, 7.34Mb/s]                                                 \n",
            "Fetching model.ckpt.meta: 472kb [00:00, 1.67Mb/s]                                                   \n",
            "Fetching vocab.bpe: 457kb [00:00, 1.56Mb/s]                                                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MODEL_SIZE = \"124M\"  # Choose from: \"124M\", \"355M\", \"774M\", \"1558M\"\n",
        "MODELS_DIR = \"models\"  # Directory to store downloaded models\n",
        "\n",
        "# Load encoder (tokenizer), hyperparameters, and model parameters\n",
        "print(f\"Loading GPT-2 {MODEL_SIZE} model...\")\n",
        "encoder, hparams, params = load_encoder_hparams_and_params(MODEL_SIZE, MODELS_DIR)\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "da02c04d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da02c04d",
        "outputId": "f624376f-2037-4a42-f412-7853257237a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Hyperparameters:\n",
            "  - Number of layers (n_layer): 12\n",
            "  - Number of attention heads (n_head): 12\n",
            "  - Embedding dimension (n_embd): 768\n",
            "  - Vocabulary size (n_vocab): 50257\n",
            "  - Context length (n_ctx): 1024\n"
          ]
        }
      ],
      "source": [
        "# Display model hyperparameters\n",
        "print(\"Model Hyperparameters:\")\n",
        "print(f\"  - Number of layers (n_layer): {hparams['n_layer']}\")\n",
        "print(f\"  - Number of attention heads (n_head): {hparams['n_head']}\")\n",
        "print(f\"  - Embedding dimension (n_embd): {hparams['n_embd']}\")\n",
        "print(f\"  - Vocabulary size (n_vocab): {hparams['n_vocab']}\")\n",
        "print(f\"  - Context length (n_ctx): {hparams['n_ctx']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "770f497c",
      "metadata": {
        "id": "770f497c"
      },
      "source": [
        "## 7. Run Inference\n",
        "\n",
        "Now let's generate some text! You can modify the prompt and the number of tokens to generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "13303e7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13303e7b",
        "outputId": "cbe95fbc-c546-40b7-a926-2033ab6c52f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Alan Turing theorized that computers would one day become\n",
            "Generating 40 tokens...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Input prompt\n",
        "prompt = \"Alan Turing theorized that computers would one day become\"\n",
        "\n",
        "# Number of tokens to generate\n",
        "n_tokens_to_generate = 40\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generating {n_tokens_to_generate} tokens...\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ec71058d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec71058d",
        "outputId": "4ef293c5-1e31-493f-a3a0-c04ba02c7dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input token IDs: [36235, 39141, 18765, 1143, 326, 9061, 561, 530, 1110, 1716]\n",
            "Number of input tokens: 10\n"
          ]
        }
      ],
      "source": [
        "# Encode the input prompt\n",
        "input_ids = encoder.encode(prompt)\n",
        "print(f\"Input token IDs: {input_ids}\")\n",
        "print(f\"Number of input tokens: {len(input_ids)}\")\n",
        "\n",
        "# Make sure we don't exceed the context length\n",
        "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"], \\\n",
        "    f\"Total tokens ({len(input_ids) + n_tokens_to_generate}) exceeds context length ({hparams['n_ctx']})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "43b9ef51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43b9ef51",
        "outputId": "b35dd109-017d-49b7-9a1f-d4d9eb3f573a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 40/40 [01:14<00:00,  1.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Generated Text:\n",
            "==================================================\n",
            "Alan Turing theorized that computers would one day become the most powerful machines on the planet.\n",
            "\n",
            "The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate output tokens\n",
        "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
        "\n",
        "# Decode the generated tokens back to text\n",
        "output_text = encoder.decode(output_ids)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Generated Text:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"{prompt}{output_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e6fca20",
      "metadata": {
        "id": "6e6fca20"
      },
      "source": [
        "## 8. Interactive Generation\n",
        "\n",
        "Try different prompts below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8aff5314",
      "metadata": {
        "id": "8aff5314"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, n_tokens=40):\n",
        "    \"\"\"Helper function to generate text from a prompt.\"\"\"\n",
        "    input_ids = encoder.encode(prompt)\n",
        "\n",
        "    if len(input_ids) + n_tokens >= hparams[\"n_ctx\"]:\n",
        "        print(f\"Warning: Reducing tokens to fit context length\")\n",
        "        n_tokens = hparams[\"n_ctx\"] - len(input_ids) - 1\n",
        "\n",
        "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens)\n",
        "    output_text = encoder.decode(output_ids)\n",
        "\n",
        "    return prompt + output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0c77da7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c77da7e",
        "outputId": "776ea070-34ec-407a-f405-3e3a36022489"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 50/50 [01:29<00:00,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The future of artificial intelligence is uncertain.\n",
            "\n",
            "\"We're not sure what the future will look like,\" said Dr. Michael S. Schoenfeld, a professor of computer science at the University of California, Berkeley. \"But we're not sure what the future will look\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Try your own prompts!\n",
        "my_prompt = \"The future of artificial intelligence is\"\n",
        "result = generate_text(my_prompt, n_tokens=50)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "86f3f930",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f3f930",
        "outputId": "0a59e19c-4934-4c21-97e5-5c59c389123e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 50/50 [01:27<00:00,  1.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In a world where robots are becoming more and more commonplace, it's important to remember that robots are not just a threat to humanity, but also to the planet.\n",
            "\n",
            "The robots that are currently in use are the ones that are currently being used to make the most of\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Another example\n",
        "my_prompt = \"In a world where robots\"\n",
        "result = generate_text(my_prompt, n_tokens=50)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e6a8d7",
      "metadata": {
        "id": "71e6a8d7"
      },
      "source": [
        "## 9. Understanding the Model Architecture\n",
        "\n",
        "Let's explore the model's parameters to better understand its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ff163927",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff163927",
        "outputId": "0fa09c93-d1ce-4a5d-8b1c-73d7f71fc788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-level parameters:\n",
            "  ln_f: ['b', 'g']\n",
            "  wpe: shape = (1024, 768)\n",
            "  wte: shape = (50257, 768)\n",
            "\n",
            "Number of transformer blocks: 12\n"
          ]
        }
      ],
      "source": [
        "# Explore the parameter structure\n",
        "print(\"Top-level parameters:\")\n",
        "for key in params.keys():\n",
        "    if key != 'blocks':\n",
        "        if isinstance(params[key], dict):\n",
        "            print(f\"  {key}: {list(params[key].keys())}\")\n",
        "        else:\n",
        "            print(f\"  {key}: shape = {params[key].shape}\")\n",
        "\n",
        "print(f\"\\nNumber of transformer blocks: {len(params['blocks'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "193f1b95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "193f1b95",
        "outputId": "4ab098bb-77b5-40bd-b3c3-d11dca46045e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structure of a transformer block:\n",
            "  attn:\n",
            "    c_attn:\n",
            "      b: shape = (2304,)\n",
            "      w: shape = (768, 2304)\n",
            "    c_proj:\n",
            "      b: shape = (768,)\n",
            "      w: shape = (768, 768)\n",
            "  ln_1:\n",
            "    b: shape = (768,)\n",
            "    g: shape = (768,)\n",
            "  ln_2:\n",
            "    b: shape = (768,)\n",
            "    g: shape = (768,)\n",
            "  mlp:\n",
            "    c_fc:\n",
            "      b: shape = (3072,)\n",
            "      w: shape = (768, 3072)\n",
            "    c_proj:\n",
            "      b: shape = (768,)\n",
            "      w: shape = (3072, 768)\n"
          ]
        }
      ],
      "source": [
        "# Explore a single transformer block\n",
        "block = params['blocks'][0]\n",
        "print(\"Structure of a transformer block:\")\n",
        "for key, value in block.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"  {key}:\")\n",
        "        for k, v in value.items():\n",
        "            # Check if v is also a dictionary, if so, iterate further\n",
        "            if isinstance(v, dict):\n",
        "                print(f\"    {k}:\")\n",
        "                for sub_k, sub_v in v.items():\n",
        "                    if isinstance(sub_v, np.ndarray):\n",
        "                        print(f\"      {sub_k}: shape = {sub_v.shape}\")\n",
        "                    else:\n",
        "                        print(f\"      {sub_k}: {type(sub_v)}\")\n",
        "            elif isinstance(v, np.ndarray): # if v is a numpy array directly\n",
        "                print(f\"    {k}: shape = {v.shape}\")\n",
        "            else: # Fallback for unexpected types for v\n",
        "                print(f\"    {k}: {type(v)}\")\n",
        "    elif isinstance(value, np.ndarray): # if value is a numpy array directly\n",
        "        print(f\"  {key}: shape = {value.shape}\")\n",
        "    else: # Fallback for unexpected types for value\n",
        "        print(f\"  {key}: {type(value)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ad53ac2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad53ac2a",
        "outputId": "8942d0c1-bd5e-4366-9a1d-ea760493fa05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 124,439,808\n",
            "Approximately: 124.4M parameters\n"
          ]
        }
      ],
      "source": [
        "# Calculate total number of parameters\n",
        "def count_params(d):\n",
        "    total = 0\n",
        "    for key, value in d.items():\n",
        "        if isinstance(value, dict):\n",
        "            total += count_params(value)\n",
        "        elif isinstance(value, list):\n",
        "            for item in value:\n",
        "                if isinstance(item, dict):\n",
        "                    total += count_params(item)\n",
        "        elif isinstance(value, np.ndarray):\n",
        "            total += value.size\n",
        "    return total\n",
        "\n",
        "total_params = count_params(params)\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "print(f\"Approximately: {total_params / 1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48033bdd",
      "metadata": {
        "id": "48033bdd"
      },
      "source": [
        "## 10. Tokenizer Exploration\n",
        "\n",
        "Let's see how the BPE tokenizer works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "491c13c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "491c13c5",
        "outputId": "89263b61-a66e-4146-e5b4-20b4b077f461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: 'Hello, world!'\n",
            "Tokens: [15496, 11, 995, 0]\n",
            "Decoded: 'Hello, world!'\n",
            "Number of tokens: 4\n",
            "\n",
            "Original: 'GPT-2 is a large language model.'\n",
            "Tokens: [38, 11571, 12, 17, 318, 257, 1588, 3303, 2746, 13]\n",
            "Decoded: 'GPT-2 is a large language model.'\n",
            "Number of tokens: 10\n",
            "\n",
            "Original: 'The quick brown fox jumps over the lazy dog.'\n",
            "Tokens: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
            "Decoded: 'The quick brown fox jumps over the lazy dog.'\n",
            "Number of tokens: 10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Encode and decode examples\n",
        "test_texts = [\n",
        "    \"Hello, world!\",\n",
        "    \"GPT-2 is a large language model.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = encoder.encode(text)\n",
        "    decoded = encoder.decode(tokens)\n",
        "    print(f\"Original: '{text}'\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Decoded: '{decoded}'\")\n",
        "    print(f\"Number of tokens: {len(tokens)}\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
