{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a12e2da",
   "metadata": {},
   "source": [
    "# PicoGPT Inference Notebook\n",
    "\n",
    "This notebook demonstrates how to load and run inference with GPT-2 using the picoGPT implementation.\n",
    "\n",
    "picoGPT is a minimal implementation of GPT-2 in pure NumPy, making it easy to understand the core concepts of transformer-based language models.\n",
    "\n",
    "**Features:**\n",
    "- Load pre-trained GPT-2 weights\n",
    "- BPE tokenization\n",
    "- Text generation with greedy decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f4db3",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import regex\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6cf84",
   "metadata": {},
   "source": [
    "## 2. BPE Tokenizer (Encoder)\n",
    "\n",
    "The GPT-2 tokenizer uses Byte Pair Encoding (BPE) to convert text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f98bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    \"\"\"BPE Encoder/Decoder for GPT-2.\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.errors = errors\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "        self.pat = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = \" \".join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to token IDs.\"\"\"\n",
    "        bpe_tokens = []\n",
    "        for token in regex.findall(self.pat, text):\n",
    "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        text = \"\".join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    \"\"\"Load the BPE encoder from files.\"\"\"\n",
    "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
    "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db22d5",
   "metadata": {},
   "source": [
    "## 3. Model Loading Utilities\n",
    "\n",
    "Functions to download GPT-2 weights from OpenAI and load them from TensorFlow checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2187272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gpt2_files(model_size, model_dir):\n",
    "    \"\"\"Download GPT-2 model files from OpenAI.\"\"\"\n",
    "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "    for filename in [\n",
    "        \"checkpoint\",\n",
    "        \"encoder.json\",\n",
    "        \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\",\n",
    "        \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\",\n",
    "        \"vocab.bpe\",\n",
    "    ]:\n",
    "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
    "            file_size = int(r.headers[\"content-length\"])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(\n",
    "                ncols=100,\n",
    "                desc=\"Fetching \" + filename,\n",
    "                total=file_size,\n",
    "                unit_scale=True,\n",
    "                unit=\"b\",\n",
    "            ) as pbar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n",
    "    \"\"\"Load GPT-2 parameters from TensorFlow checkpoint.\"\"\"\n",
    "    def set_in_nested_dict(d, keys, val):\n",
    "        if not keys:\n",
    "            return val\n",
    "        if keys[0] not in d:\n",
    "            d[keys[0]] = {}\n",
    "        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n",
    "        return d\n",
    "\n",
    "    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n",
    "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
    "        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
    "        name = name[len(\"model/\") :]\n",
    "        if name.startswith(\"h\"):\n",
    "            m = re.match(r\"h([0-9]+)/(.*)\", name)\n",
    "            n = int(m[1])\n",
    "            sub_name = m[2]\n",
    "            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n",
    "        else:\n",
    "            set_in_nested_dict(params, name.split(\"/\"), array)\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def load_encoder_hparams_and_params(model_size, models_dir):\n",
    "    \"\"\"Load encoder, hyperparameters, and model parameters.\"\"\"\n",
    "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    if not tf_ckpt_path:  # download files if necessary\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        download_gpt2_files(model_size, model_dir)\n",
    "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "\n",
    "    encoder = get_encoder(model_size, models_dir)\n",
    "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
    "\n",
    "    return encoder, hparams, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84949bb",
   "metadata": {},
   "source": [
    "## 4. GPT-2 Model Components\n",
    "\n",
    "These are the core building blocks of the GPT-2 architecture, implemented in pure NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit activation function.\"\"\"\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Numerically stable softmax function.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def layer_norm(x, g, b, eps: float = 1e-5):\n",
    "    \"\"\"Layer normalization.\"\"\"\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
    "    return g * x + b  # scale and offset with gamma/beta params\n",
    "\n",
    "\n",
    "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
    "    \"\"\"Linear transformation.\"\"\"\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    \"\"\"Feed-forward network (MLP) in transformer.\"\"\"\n",
    "    # project up\n",
    "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
    "\n",
    "    # project back down\n",
    "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
    "    \"\"\"Scaled dot-product attention.\"\"\"\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
    "\n",
    "\n",
    "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "    # qkv projection\n",
    "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "\n",
    "    # split into qkv\n",
    "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
    "\n",
    "    # split into heads\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
    "\n",
    "    # causal mask to hide future inputs from being attended to\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
    "\n",
    "    # perform attention over each head\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
    "\n",
    "    # merge heads\n",
    "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "\n",
    "    # out projection\n",
    "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    \"\"\"A single transformer block.\"\"\"\n",
    "    # multi-head causal self attention\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # position-wise feed forward network\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
    "    \"\"\"GPT-2 forward pass.\"\"\"\n",
    "    # token + positional embeddings\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
    "\n",
    "    # forward pass through n_layer transformer blocks\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # projection to vocab\n",
    "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1884f",
   "metadata": {},
   "source": [
    "## 5. Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0a30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    \"\"\"Generate tokens autoregressively using greedy decoding.\"\"\"\n",
    "    for _ in tqdm(range(n_tokens_to_generate), desc=\"Generating\"):\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
    "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
    "        inputs.append(int(next_id))  # append prediction to input\n",
    "\n",
    "    return inputs[len(inputs) - n_tokens_to_generate:]  # only return generated ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b81ef",
   "metadata": {},
   "source": [
    "## 6. Load Model Weights and Tokenizer\n",
    "\n",
    "We'll load the pre-trained GPT-2 model. The model will be downloaded automatically if not present.\n",
    "\n",
    "Available model sizes:\n",
    "- `124M` - Small (default)\n",
    "- `355M` - Medium\n",
    "- `774M` - Large\n",
    "- `1558M` - XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_SIZE = \"124M\"  # Choose from: \"124M\", \"355M\", \"774M\", \"1558M\"\n",
    "MODELS_DIR = \"models\"  # Directory to store downloaded models\n",
    "\n",
    "# Load encoder (tokenizer), hyperparameters, and model parameters\n",
    "print(f\"Loading GPT-2 {MODEL_SIZE} model...\")\n",
    "encoder, hparams, params = load_encoder_hparams_and_params(MODEL_SIZE, MODELS_DIR)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model hyperparameters\n",
    "print(\"Model Hyperparameters:\")\n",
    "print(f\"  - Number of layers (n_layer): {hparams['n_layer']}\")\n",
    "print(f\"  - Number of attention heads (n_head): {hparams['n_head']}\")\n",
    "print(f\"  - Embedding dimension (n_embd): {hparams['n_embd']}\")\n",
    "print(f\"  - Vocabulary size (n_vocab): {hparams['n_vocab']}\")\n",
    "print(f\"  - Context length (n_ctx): {hparams['n_ctx']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f497c",
   "metadata": {},
   "source": [
    "## 7. Run Inference\n",
    "\n",
    "Now let's generate some text! You can modify the prompt and the number of tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13303e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prompt\n",
    "prompt = \"Alan Turing theorized that computers would one day become\"\n",
    "\n",
    "# Number of tokens to generate\n",
    "n_tokens_to_generate = 40\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generating {n_tokens_to_generate} tokens...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input prompt\n",
    "input_ids = encoder.encode(prompt)\n",
    "print(f\"Input token IDs: {input_ids}\")\n",
    "print(f\"Number of input tokens: {len(input_ids)}\")\n",
    "\n",
    "# Make sure we don't exceed the context length\n",
    "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"], \\\n",
    "    f\"Total tokens ({len(input_ids) + n_tokens_to_generate}) exceeds context length ({hparams['n_ctx']})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output tokens\n",
    "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "\n",
    "# Decode the generated tokens back to text\n",
    "output_text = encoder.decode(output_ids)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Generated Text:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{prompt}{output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6fca20",
   "metadata": {},
   "source": [
    "## 8. Interactive Generation\n",
    "\n",
    "Try different prompts below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, n_tokens=40):\n",
    "    \"\"\"Helper function to generate text from a prompt.\"\"\"\n",
    "    input_ids = encoder.encode(prompt)\n",
    "    \n",
    "    if len(input_ids) + n_tokens >= hparams[\"n_ctx\"]:\n",
    "        print(f\"Warning: Reducing tokens to fit context length\")\n",
    "        n_tokens = hparams[\"n_ctx\"] - len(input_ids) - 1\n",
    "    \n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens)\n",
    "    output_text = encoder.decode(output_ids)\n",
    "    \n",
    "    return prompt + output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own prompts!\n",
    "my_prompt = \"The future of artificial intelligence is\"\n",
    "result = generate_text(my_prompt, n_tokens=50)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "my_prompt = \"In a world where robots\"\n",
    "result = generate_text(my_prompt, n_tokens=50)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6a8d7",
   "metadata": {},
   "source": [
    "## 9. Understanding the Model Architecture\n",
    "\n",
    "Let's explore the model's parameters to better understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff163927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the parameter structure\n",
    "print(\"Top-level parameters:\")\n",
    "for key in params.keys():\n",
    "    if key != 'blocks':\n",
    "        if isinstance(params[key], dict):\n",
    "            print(f\"  {key}: {list(params[key].keys())}\")\n",
    "        else:\n",
    "            print(f\"  {key}: shape = {params[key].shape}\")\n",
    "\n",
    "print(f\"\\nNumber of transformer blocks: {len(params['blocks'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a single transformer block\n",
    "block = params['blocks'][0]\n",
    "print(\"Structure of a transformer block:\")\n",
    "for key, value in block.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"    {k}: shape = {v.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: shape = {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of parameters\n",
    "def count_params(d):\n",
    "    total = 0\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            total += count_params(value)\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    total += count_params(item)\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            total += value.size\n",
    "    return total\n",
    "\n",
    "total_params = count_params(params)\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "print(f\"Approximately: {total_params / 1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48033bdd",
   "metadata": {},
   "source": [
    "## 10. Tokenizer Exploration\n",
    "\n",
    "Let's see how the BPE tokenizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and decode examples\n",
    "test_texts = [\n",
    "    \"Hello, world!\",\n",
    "    \"GPT-2 is a large language model.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = encoder.encode(text)\n",
    "    decoded = encoder.decode(tokens)\n",
    "    print(f\"Original: '{text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Decoded: '{decoded}'\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
