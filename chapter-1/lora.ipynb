{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5cdbf2",
   "metadata": {},
   "source": [
    "# LoRA (Low-Rank Adaptation) Implementation\n",
    "\n",
    "This notebook implements LoRA (Low-Rank Adaptation) for a custom Transformer block (`QWQBlockWithLoRA`) on a dummy LegalQA dataset.\n",
    "\n",
    "LoRA allows us to fine-tune large models by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalQADataset(Dataset):\n",
    "    def __init__(self, num_samples=500):\n",
    "        self.data = []\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            label = random.randint(0, 1) # Binary classification\n",
    "            q_vec = torch.randn(64) + label * 0.5  # Simple pattern based on label\n",
    "            a_vec = torch.randn(64) + label * 0.5  # Simple pattern based on label\n",
    "            self.data.append((q_vec, a_vec, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q, a, label = self.data[idx]\n",
    "        return q, a, torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abff3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAModule(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, rank=4, alpha=16):\n",
    "        super(LoRAModule, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / self.rank\n",
    "\n",
    "        # Original weight matrix (frozen during training)\n",
    "        # Shape: (out_dim, input_dim)\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, input_dim), requires_grad=False)\n",
    "\n",
    "        # Trainable low-rank adaptation matrices\n",
    "        # lora_A: (rank, input_dim)\n",
    "        # lora_B: (out_dim, rank)\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, input_dim) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.randn(out_dim, rank) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        \n",
    "        # Original path: x @ weight.T\n",
    "        # (batch_size, input_dim) @ (input_dim, out_dim) -> (batch_size, out_dim)\n",
    "        original_output = F.linear(x, self.weight) \n",
    "        \n",
    "        # LoRA path: x @ A.T @ B.T\n",
    "        # 1. x @ lora_A.T\n",
    "        # (batch_size, input_dim) @ (input_dim, rank) -> (batch_size, rank)\n",
    "        lora_output = F.linear(x, self.lora_A) \n",
    "        \n",
    "        # 2. result @ lora_B.T\n",
    "        # (batch_size, rank) @ (rank, out_dim) -> (batch_size, out_dim)\n",
    "        lora_output = F.linear(lora_output, self.lora_B) * self.scaling \n",
    "        \n",
    "        return original_output + lora_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc025db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QWQBlockWithLoRA(nn.Module):\n",
    "    def __init__(self, input_dim=64):\n",
    "        super(QWQBlockWithLoRA, self).__init__()\n",
    "        self.q_proj = LoRAModule(input_dim, input_dim)\n",
    "        self.k_proj = LoRAModule(input_dim, input_dim)\n",
    "        self.v_proj = LoRAModule(input_dim, input_dim)\n",
    "        self.out_proj = LoRAModule(input_dim, input_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            LoRAModule(input_dim, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            LoRAModule(input_dim * 2, input_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # Self-attention\n",
    "        q_proj = self.q_proj(q)\n",
    "        k_proj = self.k_proj(k)\n",
    "        v_proj = self.v_proj(v)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attn_scores = torch.matmul(q_proj, k_proj.T) / (q_proj.size(-1) ** 0.5)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_probs, v_proj)\n",
    "        # attn_output = self.out_proj(attn_output) # Optional output projection\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.norm1(q + attn_output)\n",
    "        x = self.norm2(self.ffn(x) + x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalQAClassifierWithLoRA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LegalQAClassifierWithLoRA, self).__init__()\n",
    "        self.encoder = QWQBlockWithLoRA()\n",
    "        self.classifier = nn.Linear(64, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, q, a):\n",
    "        # In this dummy example, we use the same input for q, k, v\n",
    "        q_encoded = self.encoder(q, q, q)\n",
    "        a_encoded = self.encoder(a, a, a)\n",
    "        combined = q_encoded * a_encoded # Element-wise multiplication\n",
    "        return self.classifier(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = LegalQAClassifierWithLoRA().to(device)\n",
    "\n",
    "# Only train parameters that require gradients (LoRA parameters)\n",
    "# The original weights are frozen (requires_grad=False)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loader = DataLoader(LegalQADataset(), batch_size=32, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch in loader:\n",
    "        q_batch, a_batch, labels = batch\n",
    "        q_batch, a_batch, labels = q_batch.to(device), a_batch.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(q_batch, a_batch)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    acc = correct / len(loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}, Accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
