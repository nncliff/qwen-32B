{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-1/lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5cdbf2",
      "metadata": {
        "id": "6b5cdbf2"
      },
      "source": [
        "# LoRA (Low-Rank Adaptation) Implementation\n",
        "\n",
        "This notebook implements LoRA (Low-Rank Adaptation) for a custom Transformer block (`QWQBlockWithLoRA`) on a dummy LegalQA dataset.\n",
        "\n",
        "LoRA allows us to fine-tune large models by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "750bd965",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "750bd965",
        "outputId": "dc0dc027-8626-4f0e-e4d5-02e210b968d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x799317a5f430>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a28b4483",
      "metadata": {
        "id": "a28b4483"
      },
      "outputs": [],
      "source": [
        "class LegalQADataset(Dataset):\n",
        "    def __init__(self, num_samples=500):\n",
        "        self.data = []\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            label = random.randint(0, 1) # Binary classification\n",
        "            q_vec = torch.randn(64) + label * 0.5  # Simple pattern based on label\n",
        "            a_vec = torch.randn(64) + label * 0.5 # Simple pattern based on label\n",
        "            self.data.append((q_vec, a_vec, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        q, a, label = self.data[idx]\n",
        "        return q, a, torch.tensor(label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061f2d3b",
      "metadata": {
        "id": "061f2d3b"
      },
      "source": [
        "### Understanding `F.linear` and Matrix Shapes\n",
        "\n",
        "**The Short Answer:**\n",
        "`F.linear(input, weight)` **automatically transposes** the `weight` matrix.\n",
        "It computes $y = x W^T$.\n",
        "\n",
        "**The Detailed Explanation:**\n",
        "1.  **Standard Math**: We usually write $y = xW$, where $x$ is $(1 \\times D_{in})$ and $W$ is $(D_{in} \\times D_{out})$.\n",
        "2.  **PyTorch Convention**: PyTorch stores Linear layer weights as $(D_{out} \\times D_{in})$ to optimize memory layout.\n",
        "3.  **The Operation**: Because the weight is stored \"transposed\" relative to the math notation, `F.linear` performs the transpose for you during the forward pass.\n",
        "\n",
        "**In the code below:**\n",
        "*   `x` shape: `(batch_size, input_dim)`\n",
        "*   `self.lora_A` shape: `(rank, input_dim)`\n",
        "\n",
        "When calling `F.linear(x, self.lora_A)`:\n",
        "*   It computes: $x \\cdot \\text{lora\\_A}^T$\n",
        "*   Shapes: `(batch, input_dim)` $\\cdot$ `(input_dim, rank)`\n",
        "*   Result: `(batch, rank)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5abff3af",
      "metadata": {
        "id": "5abff3af"
      },
      "outputs": [],
      "source": [
        "class LoRAModule(nn.Module):\n",
        "    def __init__(self, input_dim, out_dim, rank=4, alpha=16):\n",
        "        super(LoRAModule, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = self.alpha / self.rank\n",
        "\n",
        "        # Original weight matrix (frozen during training)\n",
        "        # Shape: (out_dim, input_dim)\n",
        "        self.weight = nn.Parameter(torch.randn(out_dim, input_dim), requires_grad=False)\n",
        "\n",
        "        # Trainable low-rank adaptation matrices\n",
        "        # lora_A: (rank, input_dim)\n",
        "        # lora_B: (out_dim, rank)\n",
        "        self.lora_A = nn.Parameter(torch.randn(rank, input_dim) * 0.01)\n",
        "        self.lora_B = nn.Parameter(torch.randn(out_dim, rank) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, input_dim)\n",
        "\n",
        "        # Original path: x @ weight.T\n",
        "        # (batch_size, input_dim) @ (input_dim, out_dim) -> (batch_size, out_dim)\n",
        "        original_output = F.linear(x, self.weight)\n",
        "\n",
        "        # LoRA path: x @ lora_A.T @ lora_B.T\n",
        "        # 1. x @ lora_A.T\n",
        "        # (batch_size, input_dim) @ (input_dim, rank) -> (batch_size, rank)\n",
        "        lora_output = F.linear(x, self.lora_A)\n",
        "\n",
        "        # 2. result @ lora_B.T\n",
        "        # (batch_size, rank) @ (rank, out_dim) -> (batch_size, out_dim)\n",
        "        lora_output = F.linear(lora_output, self.lora_B) * self.scaling\n",
        "\n",
        "        return original_output + lora_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dc025db6",
      "metadata": {
        "id": "dc025db6"
      },
      "outputs": [],
      "source": [
        "class QWQBlockWithLoRA(nn.Module):\n",
        "    def __init__(self, input_dim=64):\n",
        "        super(QWQBlockWithLoRA, self).__init__()\n",
        "        self.q_proj = LoRAModule(input_dim, input_dim)\n",
        "        self.k_proj = LoRAModule(input_dim, input_dim)\n",
        "        self.v_proj = LoRAModule(input_dim, input_dim)\n",
        "        self.out_proj = LoRAModule(input_dim, input_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            LoRAModule(input_dim, input_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            LoRAModule(input_dim * 2, input_dim)\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # Self-attention\n",
        "        q_proj = self.q_proj(q)\n",
        "        k_proj = self.k_proj(k)\n",
        "        v_proj = self.v_proj(v)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        attn_scores = torch.matmul(q_proj, k_proj.T) / (q_proj.size(-1) ** 0.5)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_probs, v_proj)\n",
        "        attn_output = self.out_proj(attn_output) # Optional output projection\n",
        "\n",
        "        # Add & Norm\n",
        "        x = self.norm1(q + attn_output)\n",
        "        x = self.norm2(self.ffn(x) + x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "923233df",
      "metadata": {},
      "source": [
        "### Q&A Classification Strategy: Dual Encoder\n",
        "\n",
        "In this `LegalQAClassifierWithLoRA`, we use a simple **Dual Encoder** architecture:\n",
        "\n",
        "1.  **Independent Encoding**:\n",
        "    *   The Question (`q`) and Answer (`a`) are processed separately by the *same* encoder (`self.encoder`).\n",
        "    *   `self.encoder(q, q, q)` applies self-attention to the question vector to generate a context-aware representation.\n",
        "    *   This results in two dense vectors: `q_encoded` and `a_encoded`.\n",
        "\n",
        "2.  **Interaction (Element-wise Multiplication)**:\n",
        "    *   `combined = q_encoded * a_encoded`\n",
        "    *   We combine the two representations using element-wise multiplication (Hadamard product). This operation emphasizes features that are prominent in *both* the question and the answer, acting as a similarity measure.\n",
        "\n",
        "3.  **Classification**:\n",
        "    *   The combined vector is passed to a linear classifier to predict the relationship (e.g., relevant vs. irrelevant).\n",
        "\n",
        "This approach is efficient and effective for simple matching tasks, as it forces the model to learn a shared embedding space where matching questions and answers have similar features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7a2bceed",
      "metadata": {
        "id": "7a2bceed"
      },
      "outputs": [],
      "source": [
        "class LegalQAClassifierWithLoRA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LegalQAClassifierWithLoRA, self).__init__()\n",
        "        self.encoder = QWQBlockWithLoRA()\n",
        "        self.classifier = nn.Linear(64, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, q, a):\n",
        "        # In this dummy example, we use the same input for q, k, v\n",
        "        q_encoded = self.encoder(q, q, q)\n",
        "        a_encoded = self.encoder(a, a, a)\n",
        "        combined = q_encoded * a_encoded # Element-wise multiplication\n",
        "        return self.classifier(combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b5c4a8bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5c4a8bf",
        "outputId": "f349f4b2-5471-412d-98ac-f84b4ed6b4ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Epoch 1, Loss: 0.7094, Accuracy: 0.5060\n",
            "Epoch 2, Loss: 0.7080, Accuracy: 0.5040\n",
            "Epoch 3, Loss: 0.7082, Accuracy: 0.5040\n",
            "Epoch 4, Loss: 0.6803, Accuracy: 0.5720\n",
            "Epoch 5, Loss: 0.6752, Accuracy: 0.5760\n",
            "Epoch 6, Loss: 0.6658, Accuracy: 0.5680\n",
            "Epoch 7, Loss: 0.6465, Accuracy: 0.6180\n",
            "Epoch 8, Loss: 0.6205, Accuracy: 0.6760\n",
            "Epoch 9, Loss: 0.5986, Accuracy: 0.6700\n",
            "Epoch 10, Loss: 0.5961, Accuracy: 0.6860\n",
            "Epoch 11, Loss: 0.5835, Accuracy: 0.7000\n",
            "Epoch 12, Loss: 0.5314, Accuracy: 0.7400\n",
            "Epoch 13, Loss: 0.5291, Accuracy: 0.7200\n",
            "Epoch 14, Loss: 0.5226, Accuracy: 0.7500\n",
            "Epoch 15, Loss: 0.5526, Accuracy: 0.7380\n",
            "Epoch 16, Loss: 0.4953, Accuracy: 0.7620\n",
            "Epoch 17, Loss: 0.5140, Accuracy: 0.7300\n",
            "Epoch 18, Loss: 0.5242, Accuracy: 0.7340\n",
            "Epoch 19, Loss: 0.5110, Accuracy: 0.7620\n",
            "Epoch 20, Loss: 0.4728, Accuracy: 0.7760\n",
            "Epoch 21, Loss: 0.4252, Accuracy: 0.7920\n",
            "Epoch 22, Loss: 0.4417, Accuracy: 0.7920\n",
            "Epoch 23, Loss: 0.3679, Accuracy: 0.8360\n",
            "Epoch 24, Loss: 0.3635, Accuracy: 0.8260\n",
            "Epoch 25, Loss: 0.3291, Accuracy: 0.8460\n",
            "Epoch 26, Loss: 0.2997, Accuracy: 0.8760\n",
            "Epoch 27, Loss: 0.3015, Accuracy: 0.8700\n",
            "Epoch 28, Loss: 0.3053, Accuracy: 0.8800\n",
            "Epoch 29, Loss: 0.2869, Accuracy: 0.8880\n",
            "Epoch 30, Loss: 0.2782, Accuracy: 0.8940\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = LegalQAClassifierWithLoRA().to(device)\n",
        "\n",
        "# Only train parameters that require gradients (LoRA parameters)\n",
        "# The original weights are frozen (requires_grad=False)\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "loader = DataLoader(LegalQADataset(), batch_size=32, shuffle=True)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(30):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for batch in loader:\n",
        "        q_batch, a_batch, labels = batch\n",
        "        q_batch, a_batch, labels = q_batch.to(device), a_batch.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(q_batch, a_batch)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    acc = correct / len(loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}, Accuracy: {acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
