{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-1/lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5cdbf2",
      "metadata": {
        "id": "6b5cdbf2"
      },
      "source": [
        "# LoRA (Low-Rank Adaptation) Implementation\n",
        "\n",
        "This notebook implements LoRA (Low-Rank Adaptation) for a custom Transformer block (`QWQBlockWithLoRA`) on a dummy LegalQA dataset.\n",
        "\n",
        "LoRA allows us to fine-tune large models by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "750bd965",
      "metadata": {
        "id": "750bd965",
        "outputId": "a5da3935-5c10-41f1-9a09-b528642ed3e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b03cce872f0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "a28b4483",
      "metadata": {
        "id": "a28b4483"
      },
      "outputs": [],
      "source": [
        "class LegalQADataset(Dataset):\n",
        "    def __init__(self, num_samples=500):\n",
        "        self.data = []\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            label = random.randint(0, 1) # Binary classification\n",
        "            q_vec = torch.randn(64) + label * 0.5  # Simple pattern based on label\n",
        "            a_vec = torch.randn(64) + label * 0.5 # Simple pattern based on label\n",
        "            self.data.append((q_vec, a_vec, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        q, a, label = self.data[idx]\n",
        "        return q, a, torch.tensor(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "5abff3af",
      "metadata": {
        "id": "5abff3af"
      },
      "outputs": [],
      "source": [
        "class LoRAModule(nn.Module):\n",
        "    def __init__(self, input_dim, out_dim, rank=4, alpha=16):\n",
        "        super(LoRAModule, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = self.alpha / self.rank\n",
        "\n",
        "        # Original weight matrix (frozen during training)\n",
        "        # Shape: (out_dim, input_dim)\n",
        "        self.weight = nn.Parameter(torch.randn(out_dim, input_dim), requires_grad=False)\n",
        "\n",
        "        # Trainable low-rank adaptation matrices\n",
        "        # lora_A: (rank, input_dim)\n",
        "        # lora_B: (out_dim, rank)\n",
        "        self.lora_A = nn.Parameter(torch.randn(rank, input_dim) * 0.01)\n",
        "        self.lora_B = nn.Parameter(torch.randn(out_dim, rank) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, input_dim)\n",
        "\n",
        "        # Original path: x @ weight.T\n",
        "        # (batch_size, input_dim) @ (input_dim, out_dim) -> (batch_size, out_dim)\n",
        "        original_output = F.linear(x, self.weight)\n",
        "\n",
        "        # LoRA path: x @ A.T @ B.T\n",
        "        # 1. x @ lora_A.T\n",
        "        # (batch_size, input_dim) @ (input_dim, rank) -> (batch_size, rank)\n",
        "        lora_output = F.linear(x, self.lora_A)\n",
        "\n",
        "        # 2. result @ lora_B.T\n",
        "        # (batch_size, rank) @ (rank, out_dim) -> (batch_size, out_dim)\n",
        "        lora_output = F.linear(lora_output, self.lora_B) * self.scaling\n",
        "\n",
        "        return original_output + lora_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "dc025db6",
      "metadata": {
        "id": "dc025db6"
      },
      "outputs": [],
      "source": [
        "class QWQBlockWithLoRA(nn.Module):\n",
        "    def __init__(self, input_dim=64):\n",
        "        super(QWQBlockWithLoRA, self).__init__()\n",
        "        self.q_proj = LoRAModule(input_dim, input_dim)\n",
        "        self.k_proj = LoRAModule(input_dim, input_dim)\n",
        "        self.v_proj = LoRAModule(input_dim, input_dim)\n",
        "        self.out_proj = LoRAModule(input_dim, input_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            LoRAModule(input_dim, input_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            LoRAModule(input_dim * 2, input_dim)\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # Self-attention\n",
        "        q_proj = self.q_proj(q)\n",
        "        k_proj = self.k_proj(k)\n",
        "        v_proj = self.v_proj(v)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        attn_scores = torch.matmul(q_proj, k_proj.T) / (q_proj.size(-1) ** 0.5)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_probs, v_proj)\n",
        "        # attn_output = self.out_proj(attn_output) # Optional output projection\n",
        "\n",
        "        # Add & Norm\n",
        "        x = self.norm1(q + attn_output)\n",
        "        x = self.norm2(self.ffn(x) + x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "7a2bceed",
      "metadata": {
        "id": "7a2bceed"
      },
      "outputs": [],
      "source": [
        "class LegalQAClassifierWithLoRA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LegalQAClassifierWithLoRA, self).__init__()\n",
        "        self.encoder = QWQBlockWithLoRA()\n",
        "        self.classifier = nn.Linear(64, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, q, a):\n",
        "        # In this dummy example, we use the same input for q, k, v\n",
        "        q_encoded = self.encoder(q, q, q)\n",
        "        a_encoded = self.encoder(a, a, a)\n",
        "        combined = q_encoded * a_encoded # Element-wise multiplication\n",
        "        return self.classifier(combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "b5c4a8bf",
      "metadata": {
        "id": "b5c4a8bf",
        "outputId": "d781691a-f98e-408e-bef6-76d9b54e9595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Epoch 1, Loss: 0.7350, Accuracy: 0.5400\n",
            "Epoch 2, Loss: 0.7249, Accuracy: 0.4960\n",
            "Epoch 3, Loss: 0.6937, Accuracy: 0.5640\n",
            "Epoch 4, Loss: 0.6792, Accuracy: 0.5560\n",
            "Epoch 5, Loss: 0.6853, Accuracy: 0.5740\n",
            "Epoch 6, Loss: 0.6752, Accuracy: 0.5900\n",
            "Epoch 7, Loss: 0.6348, Accuracy: 0.6400\n",
            "Epoch 8, Loss: 0.6271, Accuracy: 0.6520\n",
            "Epoch 9, Loss: 0.6188, Accuracy: 0.6500\n",
            "Epoch 10, Loss: 0.6343, Accuracy: 0.6400\n",
            "Epoch 11, Loss: 0.6091, Accuracy: 0.6740\n",
            "Epoch 12, Loss: 0.5603, Accuracy: 0.7320\n",
            "Epoch 13, Loss: 0.5346, Accuracy: 0.7480\n",
            "Epoch 14, Loss: 0.5169, Accuracy: 0.7640\n",
            "Epoch 15, Loss: 0.5721, Accuracy: 0.7440\n",
            "Epoch 16, Loss: 0.5142, Accuracy: 0.7360\n",
            "Epoch 17, Loss: 0.5102, Accuracy: 0.7280\n",
            "Epoch 18, Loss: 0.5271, Accuracy: 0.7300\n",
            "Epoch 19, Loss: 0.5166, Accuracy: 0.7480\n",
            "Epoch 20, Loss: 0.4659, Accuracy: 0.7940\n",
            "Epoch 21, Loss: 0.4553, Accuracy: 0.7760\n",
            "Epoch 22, Loss: 0.4346, Accuracy: 0.8120\n",
            "Epoch 23, Loss: 0.3818, Accuracy: 0.8320\n",
            "Epoch 24, Loss: 0.4064, Accuracy: 0.8040\n",
            "Epoch 25, Loss: 0.3726, Accuracy: 0.8240\n",
            "Epoch 26, Loss: 0.3859, Accuracy: 0.8460\n",
            "Epoch 27, Loss: 0.3307, Accuracy: 0.8580\n",
            "Epoch 28, Loss: 0.3515, Accuracy: 0.8320\n",
            "Epoch 29, Loss: 0.3268, Accuracy: 0.8660\n",
            "Epoch 30, Loss: 0.3322, Accuracy: 0.8700\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = LegalQAClassifierWithLoRA().to(device)\n",
        "\n",
        "# Only train parameters that require gradients (LoRA parameters)\n",
        "# The original weights are frozen (requires_grad=False)\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "loader = DataLoader(LegalQADataset(), batch_size=32, shuffle=True)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(30):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for batch in loader:\n",
        "        q_batch, a_batch, labels = batch\n",
        "        q_batch, a_batch, labels = q_batch.to(device), a_batch.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(q_batch, a_batch)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    acc = correct / len(loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}, Accuracy: {acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}