{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abe8d61",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-1/post-LN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Post-Layer Normalization (Post-LN) Transformer\n",
    "\n",
    "This notebook implements a Transformer block using **Post-Layer Normalization** (Post-LN), which was the design used in the original \"Attention Is All You Need\" paper.\n",
    "\n",
    "**Post-LN vs. Pre-LN:**\n",
    "*   **Post-LN (Original):** LayerNorm is applied *after* the residual connection: `x = Norm(x + Sublayer(x))`.\n",
    "*   **Pre-LN (Modern):** LayerNorm is applied *before* the sublayer input: `x = x + Sublayer(Norm(x))`.\n",
    "\n",
    "**Characteristics:**\n",
    "Post-LN places the LayerNorm after the residual addition. This can sometimes lead to unstable gradients during the early stages of training (requiring a learning rate warm-up), but it is theoretically capable of achieving slightly better performance if trained carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d49b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTextDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_length=32, embed_dim=128):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.data = []\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            label = random.randint(0, 1)\n",
    "\n",
    "            base = -1.0 if label == 0 else 1.0\n",
    "            feature = torch.randn(seq_length, embed_dim) + base\n",
    "            \n",
    "            self.data.append((feature, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostLayerNormTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super(PostLayerNormTransformerBlock, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        # Post-LayerNorm: Attention -> Residual -> LayerNorm\n",
    "        # 1. Calculate Attention on x (not normalized x)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # 2. Add Residual Connection\n",
    "        x = x + attn_output\n",
    "        \n",
    "        # 3. Apply LayerNorm AFTER the addition\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Post-LayerNorm: FFN -> Residual -> LayerNorm\n",
    "        # 1. Calculate FFN on x\n",
    "        ffn_output = self.ffn(x)\n",
    "        \n",
    "        # 2. Add Residual Connection\n",
    "        x = x + ffn_output\n",
    "        \n",
    "        # 3. Apply LayerNorm AFTER the addition\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19252f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=4, ff_hidden_dim=256, num_classes=2, dropout=0.1):\n",
    "        super(SimpleTransformerClassifier, self).__init__()\n",
    "        # Use the new Post-LN block\n",
    "        self.transformer_block = PostLayerNormTransformerBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, embed_dim)\n",
    "        x = self.transformer_block(x)\n",
    "        x = x.transpose(1, 2)  # (batch_size, embed_dim, seq_length)\n",
    "        x = self.pool(x).squeeze(-1)  # Global average pooling (batch_size, embed_dim)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a622a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SimpleTransformerClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(DummyTextDataset(), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / (len(train_loader.dataset))\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
