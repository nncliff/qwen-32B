{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-1/ipynb/rmsnorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af52ed11",
      "metadata": {
        "id": "af52ed11"
      },
      "source": [
        "# RMSNorm Implementation (Qwen/Llama Style)\n",
        "\n",
        "This notebook demonstrates a manual implementation of RMSNorm, which is a core component used in modern LLMs like Qwen and Llama.\n",
        "\n",
        "## Key Ideas of RMSNorm\n",
        "\n",
        "1. **Simplified Computation**: Standard LayerNorm uses `(x - mean) / std`. RMSNorm argues that \"centering\" (subtracting the mean) is not important for LLM activation distributions, and actually adds computational overhead. Simply doing Scaling to stabilize values is enough. This makes computation faster on GPUs.\n",
        "\n",
        "2. **Clean Gradient Flow**: This structure ensures that the gradients in the residual stream are very clean and can flow directly to the bottom layers. This is the key reason why modern large models can stack dozens of layers without collapsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "197f7a5e",
      "metadata": {
        "id": "197f7a5e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0baa35d7",
      "metadata": {
        "id": "0baa35d7"
      },
      "source": [
        "## 1. Manual RMSNorm Implementation (Core component of Qwen/Llama)\n",
        "\n",
        "RMSNorm (Root Mean Square Layer Normalization) is a simplification of LayerNorm that:\n",
        "- Only has a scaling parameter gamma (weight), no bias term beta\n",
        "- Uses the formula: `x / sqrt(mean(x^2) + eps)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c2a1f3b1",
      "metadata": {
        "id": "c2a1f3b1"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # RMSNorm typically only has a scaling parameter gamma (weight), no bias term beta\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        # Formula: x / sqrt(mean(x^2) + eps)\n",
        "        # The mean is computed over the last dimension (dim)\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First normalize, then multiply by the learnable scaling parameter\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d2566f",
      "metadata": {
        "id": "26d2566f"
      },
      "source": [
        "## 2. Qwen-style Block (Pre-LN + RMSNorm)\n",
        "\n",
        "This block implements the Pre-LN (Pre-Layer Normalization) structure used in modern transformers:\n",
        "- Uses RMSNorm instead of LayerNorm\n",
        "- Applies normalization **before** attention and FFN (Pre-LN)\n",
        "- Key formulas:\n",
        "  - Attention: `x = x + Attention(Norm(x))`\n",
        "  - FFN: `x = x + FFN(Norm(x))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "82deb21b",
      "metadata": {
        "id": "82deb21b"
      },
      "outputs": [],
      "source": [
        "class QwenStyleBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Use RMSNorm instead of LayerNorm\n",
        "        self.ln1 = RMSNorm(embed_dim)\n",
        "        self.ln2 = RMSNorm(embed_dim)\n",
        "\n",
        "        # Attention layer\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # FFN layer (Qwen actually uses SwiGLU, but for simplicity we keep the original structure with GELU)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_hidden_dim),\n",
        "            nn.GELU(), # Modern LLMs commonly use GELU or Swish, which are better than ReLU\n",
        "            nn.Linear(ff_hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # --- Key difference: Pre-LN structure ---\n",
        "\n",
        "        # 1. Attention sub-layer\n",
        "        # Formula: x = x + Attention(Norm(x))\n",
        "        residual = x\n",
        "        x_norm = self.ln1(x) # Norm first\n",
        "        attn_output, _ = self.attention(x_norm, x_norm, x_norm)\n",
        "        x = residual + attn_output # Then residual connection\n",
        "\n",
        "        # 2. FFN sub-layer\n",
        "        # Formula: x = x + FFN(Norm(x))\n",
        "        residual = x\n",
        "        x_norm = self.ln2(x) # Norm first\n",
        "        ffn_output = self.ffn(x_norm)\n",
        "        x = residual + ffn_output # Then residual connection\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ea4bf9a",
      "metadata": {
        "id": "2ea4bf9a"
      },
      "source": [
        "## 3. Data Loading and Model Definition\n",
        "\n",
        "Below is the data loading and training code with QwenStyleBlock integrated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1928b751",
      "metadata": {
        "id": "1928b751"
      },
      "outputs": [],
      "source": [
        "class DummyTextDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, seq_length=32, embed_dim=128):\n",
        "        self.num_samples = num_samples\n",
        "        self.data = []\n",
        "        for _ in range(num_samples):\n",
        "            label = random.randint(0, 1)\n",
        "            # Simple simulation: label 0 data skews negative, label 1 data skews positive\n",
        "            base = -1.0 if label == 0 else 1.0\n",
        "            feature = torch.randn(seq_length, embed_dim) + base\n",
        "            self.data.append((feature, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8e9663f8",
      "metadata": {
        "id": "8e9663f8"
      },
      "outputs": [],
      "source": [
        "class SimpleTransformerClassifier(nn.Module):\n",
        "    def __init__(self, embed_dim=128, num_heads=4, ff_hidden_dim=256, num_classes=2, dropout=0.1):\n",
        "        super(SimpleTransformerClassifier, self).__init__()\n",
        "\n",
        "        # [Modification] Using QwenStyleBlock here\n",
        "        self.transformer_block = QwenStyleBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
        "\n",
        "        # For stability in classification tasks, usually add a Norm layer before output (Final Norm)\n",
        "        self.final_norm = RMSNorm(embed_dim)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transformer_block(x)\n",
        "        x = self.final_norm(x) # Qwen/Llama typically have a Final RMSNorm before the output layer\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f91768",
      "metadata": {
        "id": "94f91768"
      },
      "source": [
        "## 4. Training\n",
        "\n",
        "Train the Qwen-style (RMSNorm + Pre-LN) model on the dummy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cb3a0714",
      "metadata": {
        "id": "cb3a0714",
        "outputId": "444f4ce3-96b3-413e-fe41-2ed697ceef0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Starting training Qwen-style (RMSNorm + Pre-LN) model...\n",
            "Epoch 1, Loss: 0.0414, Accuracy: 0.9790\n",
            "Epoch 2, Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch 3, Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch 4, Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch 5, Loss: 0.0003, Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Slightly increase dimensions for demonstration purposes\n",
        "model = SimpleTransformerClassifier(embed_dim=128, num_heads=4).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) # AdamW is usually better than Adam\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "train_loader = DataLoader(DummyTextDataset(), batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"Starting training Qwen-style (RMSNorm + Pre-LN) model...\")\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    for features, labels in train_loader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct / (len(train_loader.dataset))\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}