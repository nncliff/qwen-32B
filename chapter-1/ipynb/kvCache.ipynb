{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "367978c7",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-1/ipynb/kvCache.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db498e49",
      "metadata": {
        "id": "db498e49"
      },
      "source": [
        "# KV Cache（キー・バリューキャッシュ）の実装\n",
        "\n",
        "このノートブックでは、大規模言語モデル（LLM）推論の効率化において重要な最適化技術である**KV Cache**メカニズムを解説します。\n",
        "\n",
        "**KV Cacheとは？**\n",
        "自己回帰生成（GPTのような）では、モデルは一度に1つのトークンを生成します。キャッシュがない場合、モデルは*すべての過去のトークン*に対してアテンションのキーとバリューを毎ステップ再計算する必要があります。KV Cacheはこれらの事前計算されたキーとバリューを保存し、モデルは*最新の*トークンに対してのみ計算すればよくなります。\n",
        "\n",
        "**このノートブックの内容：**\n",
        "1.  キャッシュをサポートする`SimpleDecoderBlock`\n",
        "2.  キャッシュの更新と削除を管理する`KVCacheManager`\n",
        "3.  キャッシュがどのように使用・更新されるかを示すマルチターン会話のシミュレーション"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b07303",
      "metadata": {
        "id": "e8b07303"
      },
      "source": [
        "### `k`と`v`には何が格納されているのか？\n",
        "\n",
        "**FFNの結果が格納されているのか？**\n",
        "**このコードでは：はい。**\n",
        "`forward`メソッドを見てみましょう：\n",
        "```python\n",
        "x = self.ffn(self.norm2(x))\n",
        "if use_cache:\n",
        "    return x, (x.clone(), x.clone())\n",
        "```\n",
        "このコードは`x`（FFN/ブロックの出力）をキャッシュに保存しています。\n",
        "\n",
        "**3. これは標準的なTransformerの動作なのか？**\n",
        "**いいえ、これは簡略化されたデモンストレーションです。**\n",
        "実際のTransformer（GPTやLlamaなど）では：\n",
        "*   KV Cacheは Attention層内部の**キーとバリューの射影**を格納します。\n",
        "*   FFNの出力は格納**しません**。\n",
        "*   ブロックの出力は格納**しません**。\n",
        "\n",
        "*注：「本物の」KV cacheを実装するには、`nn.MultiheadAttention`を使用せず、カスタムAttention層を書く必要があります。PyTorchのモジュールは内部のK/V射影を隠蔽しているためです。*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7394b14f",
      "metadata": {
        "id": "7394b14f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import string\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9891377",
      "metadata": {
        "id": "d9891377"
      },
      "outputs": [],
      "source": [
        "class SimpleDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int):\n",
        "        super(SimpleDecoderBlock, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, kv_cache = None, use_cache: bool = True) -> torch.Tensor:\n",
        "        # x: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            # Use cached key and value tensors for efficient decoding\n",
        "            k, v = kv_cache\n",
        "            x_attn, _ = self.self_attn(self.norm1(x), k, v, need_weights=False)\n",
        "        else:\n",
        "            # Compute self-attention normally\n",
        "            x_attn, _ = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), need_weights=False)\n",
        "\n",
        "        x = x + x_attn\n",
        "        x = self.ffn(self.norm2(x))\n",
        "\n",
        "        if use_cache:\n",
        "            # Update kv_cache with new key and value tensors\n",
        "            return x, (x.clone().detach(), x.clone().detach())\n",
        "\n",
        "        return x, None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ebb16f",
      "metadata": {
        "id": "16ebb16f"
      },
      "source": [
        "### `forward`におけるKV Cacheの動作\n",
        "\n",
        "`if kv_cache is not None:`ブロックが最適化の核心です。\n",
        "\n",
        "1.  **最初の呼び出し（プリフィル）**：モデルが初めてプロンプトを見るとき、`kv_cache`は通常`None`です。モデルはすべてのプロンプトトークンを並列に処理します（`else`ブロック）。\n",
        "2.  **後続の呼び出し（デコーディング）**：新しいトークンを生成するとき（または会話の新しいターンを処理するとき）、`x`として**新しいトークンのみ**を渡します。\n",
        "    *   履歴全体のアテンションを再計算する代わりに、事前計算されたキーとバリューの行列を`kv_cache`経由で提供します。\n",
        "    *   モデルは新しい`x`（クエリ）を`kv_cache`内の履歴（キー/バリュー）に対してアテンドします。\n",
        "\n",
        "**別のバッチなのか？**\n",
        "通常、いいえ。推論では「バッチサイズ」は並列に生成している独立したシーケンスの数を指します。\n",
        "*   `x`は*現在の*バッチの**新しいタイムステップ**を表します。\n",
        "*   `kv_cache`は*現在の*バッチの**過去のタイムステップ**を表します。\n",
        "\n",
        "全く関連のないリクエスト（別のユーザー）を処理する場合は、確かに空のキャッシュから始めます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23e97aa6",
      "metadata": {
        "id": "23e97aa6"
      },
      "source": [
        "### 例：チャットボットの会話\n",
        "\n",
        "「トークン生成」と「新しいターン」を明確にするため、会話で視覚化してみましょう。\n",
        "\n",
        "**1. ターン1（ユーザーが「Hi」と言う）→ プリフィルフェーズ**\n",
        "*   **入力（`x`）：** \"Hi\"（プロンプト全体）\n",
        "*   **キャッシュ：** 空（`None`）\n",
        "*   **動作：** モデルは\"Hi\"をゼロから処理。\n",
        "*   **結果：**\n",
        "    *   最初のトークンを予測：\"Hel\"\n",
        "    *   キャッシュを返す：KV(\"Hi\")\n",
        "\n",
        "**2. 応答生成（AIが「Hello」を続ける）→ デコーディングフェーズ**\n",
        "*   **入力（`x`）：** \"Hel\"（**新しく生成された**トークンのみ）\n",
        "*   **キャッシュ：** KV(\"Hi\")（ステップ1から）\n",
        "*   **動作：** モデルは\"Hel\"をキャッシュされた\"Hi\"に対してアテンド。\n",
        "*   **結果：**\n",
        "    *   次のトークンを予測：\"lo\"\n",
        "    *   キャッシュを返す：KV(\"Hi\", \"Hel\")\n",
        "\n",
        "**3. ターン2（ユーザーが「How are you?」と言う）→ 新しいターンのプリフィル**\n",
        "*   **コンテキスト：** 履歴は\"Hi\"（ユーザー）+ \"Hello\"（AI）。\n",
        "*   **入力（`x`）：** \"How are you?\"（新しいユーザー入力）\n",
        "*   **キャッシュ：** KV(\"Hi\", \"Hello\")（前のターンから保存）\n",
        "*   **動作：**\n",
        "    *   \"Hi\"と\"Hello\"を再処理**しません**。\n",
        "    *   `x=\"How are you?\"`と`kv_cache=KV(\"Hi\", \"Hello\")`を渡します。\n",
        "    *   モデルは\"How are you?\"のアテンションを計算し、キャッシュされた\"Hi\"と\"Hello\"にアテンドします。\n",
        "\n",
        "このノートブックのループでは、`round_id=2`はまさに**ターン2**と同じです。新しいトークン（`token_tensors`）を入力しますが、履歴（`kv_cache`）を提供することで、モデルは再計算せずにコンテキストを理解できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733ca968",
      "metadata": {
        "id": "733ca968"
      },
      "outputs": [],
      "source": [
        "class KVCacheManager:\n",
        "    def __init__(self, max_cache_size: int = 64):\n",
        "        self.cache : List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "        self.token_labels : List[str] = [] # To store labels for each token in the cache\n",
        "        self.max_cache_size = max_cache_size\n",
        "\n",
        "    def get_cache(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        if not self.cache:\n",
        "            return None\n",
        "\n",
        "        k = torch.cat([item[0] for item in self.cache], dim=1)  # Concatenate along sequence length\n",
        "        v = torch.cat([item[1] for item in self.cache], dim=1)  # Concatenate along sequence length\n",
        "        return (k, v) # shape of k or v: (batch_size, total_sequence_length, embed_dim)\n",
        "\n",
        "    def update_cache(self, new_kv: Tuple[torch.Tensor, torch.Tensor], tokens : List[str], current_round : int):\n",
        "        self.cache.append(new_kv)\n",
        "        self.token_labels += [f\"Round{current_round}\"] * new_kv[0].size(1)  # Assuming new_kv[0] shape is (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        if len(self.token_labels) > self.max_cache_size:\n",
        "            # Keep only current round tokens if cache is full\n",
        "            # Note: The original logic was trying to filter based on labels.\n",
        "            # Since we append new_kv (current round) at the end, and we want to keep \"Round{current_round}\",\n",
        "            # we can simply keep the last element of the cache if we assume previous rounds are what we want to discard.\n",
        "\n",
        "            # Simplified logic to avoid tensor unpacking errors from original code\n",
        "            self.cache = [self.cache[-1]]\n",
        "            self.token_labels = [label for label in self.token_labels if label == f\"Round{current_round}\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c40d8c4",
      "metadata": {
        "id": "0c40d8c4"
      },
      "outputs": [],
      "source": [
        "def generate_tokens(prompt : str, vocab : List[str], num_tokens: int = 5) -> List[str]:\n",
        "    return [random.choice(vocab) for _ in range(num_tokens)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed47c7a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed47c7a8",
        "outputId": "ee2c8d30-a27d-4eac-c921-919044177bbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Round 1 generated tokens: b o r s f\n",
            "Round 1 summary: jbdnwhasmr\n",
            "Round 2 generated tokens: o q x z a\n",
            "Round 2 summary: rmutdasaeh\n",
            "Round 3 generated tokens: g j y x t\n",
            "Round 3 summary: jbxluxwkrr\n",
            "Round 4 generated tokens: v k v f m\n",
            "Round 4 summary: eaiafvpwcw\n",
            "Round 5 generated tokens: d h b x d\n",
            "Round 5 summary: mvyfrugpva\n",
            "\n",
            "=== Final KV Cache State ===\n",
            "Current token number in cache: 25\n",
            "Round labels in cache: ['Round1', 'Round1', 'Round1', 'Round1', 'Round1', 'Round2', 'Round2', 'Round2', 'Round2', 'Round2', 'Round3', 'Round3', 'Round3', 'Round3', 'Round3', 'Round4', 'Round4', 'Round4', 'Round4', 'Round4', 'Round5', 'Round5', 'Round5', 'Round5', 'Round5']\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "decoder = SimpleDecoderBlock(embed_dim=64, num_heads=4).to(device)\n",
        "kv_cache_manager = KVCacheManager(max_cache_size=30)\n",
        "vocab = list(string.ascii_lowercase)  # Example vocabulary\n",
        "\n",
        "for round_id in range(1, 6):\n",
        "    prompt = f\"[Round {round_id}] User Input: write an function\"\n",
        "    tokens = generate_tokens(prompt, vocab)\n",
        "    print(f\"Round {round_id} generated tokens: {' '.join(tokens)}\")\n",
        "\n",
        "    # Simulate token embeddings\n",
        "    token_tensors = torch.stack([torch.randn(64) for _ in tokens]).unsqueeze(0).to(device)  # shape: (1, seq_len, embed_dim)\n",
        "\n",
        "    # Retrieve kv_cache and decode\n",
        "    kv_cache = kv_cache_manager.get_cache()\n",
        "    output, new_kv = decoder(token_tensors, kv_cache=kv_cache, use_cache=True)\n",
        "\n",
        "    if new_kv is not None:\n",
        "        kv_cache_manager.update_cache(new_kv, tokens, current_round=round_id)\n",
        "\n",
        "    summary = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
        "    print(f\"Round {round_id} summary: {summary}\")\n",
        "\n",
        "print(\"\\n=== Final KV Cache State ===\")\n",
        "print(f\"Current token number in cache: {len(kv_cache_manager.token_labels)}\")\n",
        "print(f\"Round labels in cache: {kv_cache_manager.token_labels}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
