{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-1/ipynb/query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc53fde",
      "metadata": {
        "id": "2cc53fde"
      },
      "source": [
        "# ハッシュベース検索による長文ドキュメントQAシミュレーション\n",
        "\n",
        "## 目的\n",
        "このノートブックでは、簡略化された**検索拡張生成（RAG）**パイプラインをデモンストレーションします。大規模なQAシステムでは、長いドキュメント全体をモデルに入力するのは非効率的です。代わりに、以下の手順を行います：\n",
        "1.  ドキュメントのチャンク（段落）を**インデックス化**する。\n",
        "2.  与えられたクエリに対して最も関連性の高いチャンクのみを**検索**する。\n",
        "3.  クエリと取得したコンテキストを使用して回答を**生成**する。\n",
        "\n",
        "## コード概要\n",
        "このシミュレーションは3つの主要コンポーネントで構成されています：\n",
        "\n",
        "1.  **`generate_fake_document`**:\n",
        "    - ランダムな埋め込みベクトルを生成してドキュメントをシミュレートします。\n",
        "    - 各「段落」は中心テーマ（`base`）周辺のベクトルクラスタです。\n",
        "\n",
        "2.  **`HashIndex`**:\n",
        "    - シンプルな検索インデックスを実装します。\n",
        "    - ベクトル値に基づくハッシュ関数を使用して、類似したベクトルを同じバケットにグループ化します。\n",
        "    - **検索**: クエリをハッシュして関連するバケットを見つけ、そのバケット内で正確なコサイン類似度を計算してtop-kのマッチを見つけます。これは**局所性敏感ハッシュ（LSH）**や転置ファイルインデックスを模倣しています。\n",
        "\n",
        "3.  **`LongDocQA` モデル**:\n",
        "    - 以下を含むダミーニューラルネットワーク：\n",
        "        - `query_encoder`: 入力クエリをエンコードします。\n",
        "        - `answer_decoder`: 取得したコンテキストを「読み取る」GRUベースのデコーダー。\n",
        "        - `output_proj`: 隠れ状態を回答ベクトルに射影します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3e03ae",
      "metadata": {
        "id": "bd3e03ae"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import string\n",
        "from typing import List, Dict, Tuple\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d60d97",
      "metadata": {
        "id": "48d60d97"
      },
      "outputs": [],
      "source": [
        "def generate_fake_document(num_paragraphs: int = 5, tokens_per_paragraph: int = 10, dim: int = 128) -> List[torch.Tensor]:\n",
        "    \"\"\"Generates a fake document with random sentences.\"\"\"\n",
        "    document = []\n",
        "    for _ in range(num_paragraphs):\n",
        "        base = torch.randn(dim) # shape: (dim,)\n",
        "\n",
        "        # broadcast to create a paragraph\n",
        "        paragraph = base + 0.01 * torch.randn(tokens_per_paragraph, dim) # shape: (tokens_per_paragraph, dim)\n",
        "\n",
        "        document.append(paragraph)\n",
        "\n",
        "    return document # List of tensors representing paragraphs of shape (tokens_per_paragraph, dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f5927b0",
      "metadata": {
        "id": "4f5927b0"
      },
      "source": [
        "### テンソル変換に関する注意\n",
        "以下の`_hash_vector`メソッドでは、次のコードが使用されています：\n",
        "```python\n",
        "h = hashlib.md5(vector[:3].detach().numpy().tobytes()).hexdigest()\n",
        "```\n",
        "**`.detach()`が必要な理由：**\n",
        "インデックスに渡されるベクトルは、多くの場合モデル（`query_encoder`など）から来るため、`requires_grad=True`が設定されています。PyTorchでは、勾配を持つテンソルを直接NumPy配列に変換することは許可されていません。これは計算グラフを壊すためです。`.detach()`は勾配を必要としないテンソルのビューを作成します。\n",
        "\n",
        "**`.cpu()`だけでは不十分な理由：**\n",
        "GPUで実行している場合、NumPyに変換する前に`.cpu()`を呼び出してデータをホストメモリに移動する必要があります。しかし、`.cpu()`は勾配履歴を**保持**します。そのため、テンソルが勾配を必要とする場合、`tensor.cpu().numpy()`は依然として失敗します。`.detach()`を使用する必要があり（GPUの場合は`.cpu()`も併用）、`tensor.detach().cpu().numpy()`のようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f845e6",
      "metadata": {
        "id": "77f845e6"
      },
      "outputs": [],
      "source": [
        "class HashIndex:\n",
        "    def __init__(self, key_vectors: List[torch.Tensor], num_buckets: int = 16):\n",
        "        self.buckets: Dict[int, List[Tuple[int, torch.Tensor]]] = {i: [] for i in range(num_buckets)}\n",
        "        self.num_buckets = num_buckets\n",
        "\n",
        "        for idx, paragraph in enumerate(key_vectors):\n",
        "            # Hash is based on the mean vector of the paragraph\n",
        "            key = paragraph.mean(dim=0)  # the shape of key: (dim,)\n",
        "\n",
        "            bucket_id = self._hash_vector(key) # shape: ()\n",
        "            self.buckets[bucket_id].append((idx, key))\n",
        "\n",
        "    def _hash_vector(self, vector: torch.Tensor) -> int:\n",
        "        \"\"\"Hashes a vector to a bucket ID.\"\"\"\n",
        "        h = hashlib.md5(vector[:3].detach().numpy().tobytes()).hexdigest()  # Use first 3 elements for hashing\n",
        "        return int(h, 16) % self.num_buckets\n",
        "\n",
        "    def search(self, query_vector: torch.Tensor, top_k: int = 3) -> List[int]:\n",
        "        \"\"\"Searches for the top_k closest key vectors to the query_vector.\"\"\"\n",
        "        bucket_id = self._hash_vector(query_vector)\n",
        "        candidates = self.buckets[bucket_id]\n",
        "        scores = []\n",
        "\n",
        "        for idx, vector in candidates:\n",
        "            # cosine similarity is expecting 2D tensors (batch_size, dim)\n",
        "            # .item() to get scalar value from tensor\n",
        "            dist = F.cosine_similarity(query_vector.unsqueeze(0), vector.unsqueeze(0)).item()\n",
        "            scores.append((idx, dist))\n",
        "\n",
        "        # Get top_k closest\n",
        "        scores.sort(key=lambda x: x[1])  # Sort by similarity\n",
        "        return [idx for idx, _ in scores[:top_k]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3700ed1e",
      "metadata": {
        "id": "3700ed1e"
      },
      "outputs": [],
      "source": [
        "class LongDocQA(nn.Module):\n",
        "    def __init__(self, dim: int = 128):\n",
        "        super(LongDocQA, self).__init__()\n",
        "\n",
        "        self.query_encoder = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.answer_decoder = nn.GRU(input_size=dim, hidden_size=dim, batch_first=True)\n",
        "        self.output_proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, query_vector: torch.Tensor, context_vectors: List[torch.Tensor]) -> torch.Tensor:\n",
        "        input_seq = torch.stack(context_vectors, dim=0).unsqueeze(0)  # shape: (1, num_contexts, dim)\n",
        "        _, hidden = self.answer_decoder(input_seq)  # shape: (1, 1, dim)\n",
        "        response = self.output_proj(hidden.squeeze(0))  # shape: (1, dim)\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23b1a76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d23b1a76",
        "outputId": "4dea4b97-afb5-466e-ce91-79574d092932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Turn 1: Top paragraphs indices: [25, 4, 14]\n",
            "Generate answer summary (norm): 1.6464 with Answer: X26Y9W\n",
            "\n",
            "Turn 2: Top paragraphs indices: [1, 20]\n",
            "Generate answer summary (norm): 1.3438 with Answer: SWXQVJ\n",
            "\n",
            "Turn 3: Top paragraphs indices: [29, 12, 26]\n",
            "Generate answer summary (norm): 1.5655 with Answer: 0W6VIK\n",
            "\n",
            "Turn 4: Top paragraphs indices: [17, 7, 0]\n",
            "Generate answer summary (norm): 1.5160 with Answer: A2REWG\n",
            "\n",
            "Turn 5: Top paragraphs indices: [17, 22, 21]\n",
            "Generate answer summary (norm): 1.1919 with Answer: OGPFHB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Construct fake document and index\n",
        "torch.manual_seed(42)\n",
        "dim = 64\n",
        "doc = generate_fake_document(num_paragraphs=30, tokens_per_paragraph=8, dim=dim)\n",
        "#index = HashIndex(doc_summary_vector, num_buckets=8)\n",
        "index = HashIndex(doc, num_buckets=8)\n",
        "\n",
        "# Simulate multiple turns of user queries\n",
        "model = LongDocQA(dim=dim)\n",
        "model.eval()\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Simulate a user query (main topic and paragraph are relevant)\n",
        "    base_para = random.choice(doc)\n",
        "    query_vector = base_para.mean(dim=0) + 0.02 * torch.randn(dim)  # shape: (dim,)\n",
        "    query_encoded = model.query_encoder(query_vector)  # shape: (dim,)\n",
        "\n",
        "    # Search for relevant paragraphs\n",
        "    top_indices = index.search(query_encoded, top_k=3)\n",
        "    selected_paragraphs = [doc[idx].mean(dim=0) for idx in top_indices]  # List of tensors of shape (dim,)\n",
        "\n",
        "    # Generate answer\n",
        "    with torch.no_grad():\n",
        "        answer_vector = model(query_encoded, selected_paragraphs)  # shape: (dim,)\n",
        "\n",
        "    keywords = ''.join(random.choices(string.ascii_uppercase + string.digits, k=6))\n",
        "    print(f\"Turn {i}: Top paragraphs indices: {top_indices}\")\n",
        "    print(f\"Generate answer summary (norm): {answer_vector.norm().item():.4f} with Answer: {keywords}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}