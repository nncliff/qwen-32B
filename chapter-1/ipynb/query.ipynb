{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-1/ipynb/query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc53fde",
      "metadata": {
        "id": "2cc53fde"
      },
      "source": [
        "# Long Document QA Simulation with Hash-based Retrieval\n",
        "\n",
        "## Purpose\n",
        "This notebook demonstrates a simplified **Retrieval-Augmented Generation (RAG)** pipeline. In large-scale QA systems, it is inefficient to feed an entire long document into a model. Instead, we:\n",
        "1.  **Index** the document chunks (paragraphs).\n",
        "2.  **Retrieve** only the most relevant chunks for a given query.\n",
        "3.  **Generate** an answer using the query and the retrieved context.\n",
        "\n",
        "## Code Overview\n",
        "The simulation consists of three main components:\n",
        "\n",
        "1.  **`generate_fake_document`**:\n",
        "    - Simulates a document by generating random embedding vectors.\n",
        "    - Each \"paragraph\" is a cluster of vectors around a central theme (`base`).\n",
        "\n",
        "2.  **`HashIndex`**:\n",
        "    - Implements a simple retrieval index.\n",
        "    - Uses a hash function (based on the vector values) to bucket similar vectors together.\n",
        "    - **Search**: Hashes the query to find the relevant bucket, then performs exact cosine similarity within that bucket to find the top-k matches. This mimics **Locality Sensitive Hashing (LSH)** or inverted file indexing.\n",
        "\n",
        "3.  **`LongDocQA` Model**:\n",
        "    - A dummy neural network with:\n",
        "        - `query_encoder`: Encodes the input query.\n",
        "        - `answer_decoder`: A GRU-based decoder that \"reads\" the retrieved context.\n",
        "        - `output_proj`: Projects the hidden state to an answer vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bd3e03ae",
      "metadata": {
        "id": "bd3e03ae"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import string\n",
        "from typing import List, Dict, Tuple\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "48d60d97",
      "metadata": {
        "id": "48d60d97"
      },
      "outputs": [],
      "source": [
        "def generate_fake_document(num_paragraphs: int = 5, tokens_per_paragraph: int = 10, dim: int = 128) -> List[torch.Tensor]:\n",
        "    \"\"\"Generates a fake document with random sentences.\"\"\"\n",
        "    document = []\n",
        "    for _ in range(num_paragraphs):\n",
        "        base = torch.randn(dim) # shape: (dim,)\n",
        "\n",
        "        # broadcast to create a paragraph\n",
        "        paragraph = base + 0.01 * torch.randn(tokens_per_paragraph, dim) # shape: (tokens_per_paragraph, dim)\n",
        "\n",
        "        document.append(paragraph)\n",
        "\n",
        "    return document # List of tensors representing paragraphs of shape (tokens_per_paragraph, dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f5927b0",
      "metadata": {
        "id": "4f5927b0"
      },
      "source": [
        "### Note on Tensor Conversion\n",
        "In the `_hash_vector` method below, you will see:\n",
        "```python\n",
        "h = hashlib.md5(vector[:3].detach().numpy().tobytes()).hexdigest()\n",
        "```\n",
        "**Why `.detach()` is needed:**\n",
        "The vectors passed to the index often come from a model (like `query_encoder`), so they have `requires_grad=True`. PyTorch does not allow converting a tensor with gradients directly to a NumPy array because it would break the computation graph. `.detach()` creates a view of the tensor that does not require gradients.\n",
        "\n",
        "**Why `.cpu()` is not sufficient:**\n",
        "If you are running on a GPU, you must call `.cpu()` to move data to host memory before converting to NumPy. However, `.cpu()` **preserves** the gradient history. So `tensor.cpu().numpy()` will still fail if the tensor requires gradients. You must use `.detach()` (and `.cpu()` if on GPU) like `tensor.detach().cpu().numpy()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "77f845e6",
      "metadata": {
        "id": "77f845e6"
      },
      "outputs": [],
      "source": [
        "class HashIndex:\n",
        "    def __init__(self, key_vectors: List[torch.Tensor], num_buckets: int = 16):\n",
        "        self.buckets: Dict[int, List[Tuple[int, torch.Tensor]]] = {i: [] for i in range(num_buckets)}\n",
        "        self.num_buckets = num_buckets\n",
        "\n",
        "        for idx, paragraph in enumerate(key_vectors):\n",
        "            # Hash is based on the mean vector of the paragraph\n",
        "            key = paragraph.mean(dim=0)  # the shape of key: (dim,)\n",
        "\n",
        "            bucket_id = self._hash_vector(key) # shape: ()\n",
        "            self.buckets[bucket_id].append((idx, key))\n",
        "\n",
        "    def _hash_vector(self, vector: torch.Tensor) -> int:\n",
        "        \"\"\"Hashes a vector to a bucket ID.\"\"\"\n",
        "        h = hashlib.md5(vector[:3].detach().numpy().tobytes()).hexdigest()  # Use first 3 elements for hashing\n",
        "        return int(h, 16) % self.num_buckets\n",
        "\n",
        "    def search(self, query_vector: torch.Tensor, top_k: int = 3) -> List[int]:\n",
        "        \"\"\"Searches for the top_k closest key vectors to the query_vector.\"\"\"\n",
        "        bucket_id = self._hash_vector(query_vector)\n",
        "        candidates = self.buckets[bucket_id]\n",
        "        scores = []\n",
        "\n",
        "        for idx, vector in candidates:\n",
        "            # cosine similarity is expecting 2D tensors (batch_size, dim)\n",
        "            # .item() to get scalar value from tensor\n",
        "            dist = F.cosine_similarity(query_vector.unsqueeze(0), vector.unsqueeze(0)).item()\n",
        "            scores.append((idx, dist))\n",
        "\n",
        "        # Get top_k closest\n",
        "        scores.sort(key=lambda x: x[1])  # Sort by similarity\n",
        "        return [idx for idx, _ in scores[:top_k]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3700ed1e",
      "metadata": {
        "id": "3700ed1e"
      },
      "outputs": [],
      "source": [
        "class LongDocQA(nn.Module):\n",
        "    def __init__(self, dim: int = 128):\n",
        "        super(LongDocQA, self).__init__()\n",
        "\n",
        "        self.query_encoder = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.answer_decoder = nn.GRU(input_size=dim, hidden_size=dim, batch_first=True)\n",
        "        self.output_proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, query_vector: torch.Tensor, context_vectors: List[torch.Tensor]) -> torch.Tensor:\n",
        "        input_seq = torch.stack(context_vectors, dim=0).unsqueeze(0)  # shape: (1, num_contexts, dim)\n",
        "        _, hidden = self.answer_decoder(input_seq)  # shape: (1, 1, dim)\n",
        "        response = self.output_proj(hidden.squeeze(0))  # shape: (1, dim)\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d23b1a76",
      "metadata": {
        "id": "d23b1a76",
        "outputId": "4dea4b97-afb5-466e-ce91-79574d092932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Turn 1: Top paragraphs indices: [25, 4, 14]\n",
            "Generate answer summary (norm): 1.6464 with Answer: X26Y9W\n",
            "\n",
            "Turn 2: Top paragraphs indices: [1, 20]\n",
            "Generate answer summary (norm): 1.3438 with Answer: SWXQVJ\n",
            "\n",
            "Turn 3: Top paragraphs indices: [29, 12, 26]\n",
            "Generate answer summary (norm): 1.5655 with Answer: 0W6VIK\n",
            "\n",
            "Turn 4: Top paragraphs indices: [17, 7, 0]\n",
            "Generate answer summary (norm): 1.5160 with Answer: A2REWG\n",
            "\n",
            "Turn 5: Top paragraphs indices: [17, 22, 21]\n",
            "Generate answer summary (norm): 1.1919 with Answer: OGPFHB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Construct fake document and index\n",
        "torch.manual_seed(42)\n",
        "dim = 64\n",
        "doc = generate_fake_document(num_paragraphs=30, tokens_per_paragraph=8, dim=dim)\n",
        "#index = HashIndex(doc_summary_vector, num_buckets=8)\n",
        "index = HashIndex(doc, num_buckets=8)\n",
        "\n",
        "# Simulate multiple turns of user queries\n",
        "model = LongDocQA(dim=dim)\n",
        "model.eval()\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Simulate a user query (main topic and paragraph are relevant)\n",
        "    base_para = random.choice(doc)\n",
        "    query_vector = base_para.mean(dim=0) + 0.02 * torch.randn(dim)  # shape: (dim,)\n",
        "    query_encoded = model.query_encoder(query_vector)  # shape: (dim,)\n",
        "\n",
        "    # Search for relevant paragraphs\n",
        "    top_indices = index.search(query_encoded, top_k=3)\n",
        "    selected_paragraphs = [doc[idx].mean(dim=0) for idx in top_indices]  # List of tensors of shape (dim,)\n",
        "\n",
        "    # Generate answer\n",
        "    with torch.no_grad():\n",
        "        answer_vector = model(query_encoded, selected_paragraphs)  # shape: (dim,)\n",
        "\n",
        "    keywords = ''.join(random.choices(string.ascii_uppercase + string.digits, k=6))\n",
        "    print(f\"Turn {i}: Top paragraphs indices: {top_indices}\")\n",
        "    print(f\"Generate answer summary (norm): {answer_vector.norm().item():.4f} with Answer: {keywords}\\n\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}