{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42535754",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-1/pre-LN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Pre-Layer Normalization (Pre-LN) Transformer\n",
    "\n",
    "This notebook implements a Transformer block using **Pre-Layer Normalization** (Pre-LN).\n",
    "\n",
    "**Pre-LN vs. Post-LN:**\n",
    "*   **Post-LN (Original Transformer):** LayerNorm is applied *after* the residual connection: `x = Norm(x + Sublayer(x))`.\n",
    "*   **Pre-LN (Modern Standard):** LayerNorm is applied *before* the sublayer input, inside the residual branch: `x = x + Sublayer(Norm(x))`.\n",
    "\n",
    "**Why Pre-LN?**\n",
    "Pre-LN improves training stability and allows for training deeper networks without warm-up stages, as gradients flow more directly through the residual path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506520f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTextDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_length=32, embed_dim=128):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.data = []\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            label = random.randint(0, 1)\n",
    "\n",
    "            base = -1.0 if label == 0 else 1.0\n",
    "            feature = torch.randn(seq_length, embed_dim) + base\n",
    "            \n",
    "            self.data.append((feature, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super(PreLayerNormTransformerBlock, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        # Pre-LayerNorm before attention\n",
    "        # Note: In Pre-LN, we normalize x BEFORE passing it to the sublayer.\n",
    "        # The residual connection adds the original x (un-normalized) to the output.\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_output, _ = self.attention(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_output  # Residual connection\n",
    "\n",
    "        # Pre-LayerNorm before feed-forward network\n",
    "        x_norm = self.ln2(x)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = x + ffn_output  # Residual connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=4, ff_hidden_dim=256, num_classes=2, dropout=0.1):\n",
    "        super(SimpleTransformerClassifier, self).__init__()\n",
    "        self.transformer_block = PreLayerNormTransformerBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, embed_dim)\n",
    "        x = self.transformer_block(x)\n",
    "        x = x.transpose(1, 2)  # (batch_size, embed_dim, seq_length)\n",
    "        x = self.pool(x).squeeze(-1)  # Global average pooling (batch_size, embed_dim)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c48035",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SimpleTransformerClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(DummyTextDataset(), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea402c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / (len(train_loader.dataset))\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
