{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b07303",
   "metadata": {},
   "source": [
    "### What is stored in `k` and `v`?\n",
    "\n",
    "**Do they store FFN results?**\n",
    "**In this specific code: Yes.**\n",
    "If you look at the `forward` method:\n",
    "```python\n",
    "x = self.ffn(self.norm2(x))\n",
    "if use_cache:\n",
    "    return x, (x.clone(), x.clone())\n",
    "```\n",
    "The code saves `x` (the output of the FFN/Block) into the cache.\n",
    "\n",
    "**3. Is this how standard Transformers work?**\n",
    "**No, this is a simplified demonstration.**\n",
    "In a real Transformer (like GPT or Llama):\n",
    "*   The KV Cache stores the **internal Key and Value projections** inside the Attention layer.\n",
    "*   It does **not** store the output of the FFN.\n",
    "*   It does **not** store the output of the block.\n",
    "\n",
    "*Note: Implementing a \"real\" KV cache requires writing a custom Attention layer instead of using `nn.MultiheadAttention`, as the PyTorch module hides the internal K/V projections.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import string\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9891377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        super(SimpleDecoderBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv_cache = None, use_cache: bool = True) -> torch.Tensor:\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            # Use cached key and value tensors for efficient decoding\n",
    "            k, v = kv_cache\n",
    "            x_attn, _ = self.self_attn(self.norm1(x), k, v, need_weights=False)\n",
    "        else:\n",
    "            # Compute self-attention normally\n",
    "            x_attn, _ = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), need_weights=False)\n",
    "\n",
    "        x = x + x_attn\n",
    "        x = self.ffn(self.norm2(x))\n",
    "        \n",
    "        if use_cache:\n",
    "            # Update kv_cache with new key and value tensors\n",
    "            return x, (x.clone().detach(), x.clone().detach())\n",
    "\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebb16f",
   "metadata": {},
   "source": [
    "### How KV Cache Works in `forward`\n",
    "\n",
    "The `if kv_cache is not None:` block is the core of the optimization.\n",
    "\n",
    "1.  **First Call (Prefill)**: When the model sees the prompt for the first time, `kv_cache` is usually `None`. The model processes all prompt tokens in parallel (`else` block).\n",
    "2.  **Subsequent Calls (Decoding)**: When generating new tokens (or processing a new turn in a conversation), we pass the **new tokens only** as `x`.\n",
    "    *   Instead of re-calculating attention for the entire history, we provide the pre-calculated Key and Value matrices via `kv_cache`.\n",
    "    *   The model attends to the new `x` (Query) against the history in `kv_cache` (Key/Value).\n",
    "\n",
    "**Is it a different batch?**\n",
    "Usually, no. In inference, \"batch size\" refers to how many independent sequences we are generating in parallel.\n",
    "*   `x` represents the **new time steps** for the *current* batch.\n",
    "*   `kv_cache` represents the **past time steps** for the *current* batch.\n",
    "\n",
    "If you were processing a completely unrelated request (a different user), you would indeed start with an empty cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e97aa6",
   "metadata": {},
   "source": [
    "### Example: Chatbot Conversation\n",
    "\n",
    "Let's visualize this with a conversation to clarify \"Generating Tokens\" vs \"New Turn\".\n",
    "\n",
    "**1. Turn 1 (User says \"Hi\") -> Prefill Phase**\n",
    "*   **Input (`x`):** \"Hi\" (The whole prompt)\n",
    "*   **Cache:** Empty (`None`)\n",
    "*   **Action:** Model processes \"Hi\" from scratch.\n",
    "*   **Result:**\n",
    "    *   Predicts first token: \"Hel\"\n",
    "    *   Returns Cache: KV(\"Hi\")\n",
    "\n",
    "**2. Generating Response (AI continues \"Hello\") -> Decoding Phase**\n",
    "*   **Input (`x`):** \"Hel\" (Just the **newly generated** token)\n",
    "*   **Cache:** KV(\"Hi\") (From Step 1)\n",
    "*   **Action:** Model attends \"Hel\" against cached \"Hi\".\n",
    "*   **Result:**\n",
    "    *   Predicts next token: \"lo\"\n",
    "    *   Returns Cache: KV(\"Hi\", \"Hel\")\n",
    "\n",
    "**3. Turn 2 (User says \"How are you?\") -> New Turn Prefill**\n",
    "*   **Context:** The history is \"Hi\" (User) + \"Hello\" (AI).\n",
    "*   **Input (`x`):** \"How are you?\" (New user input)\n",
    "*   **Cache:** KV(\"Hi\", \"Hello\") (Saved from previous turn)\n",
    "*   **Action:**\n",
    "    *   We **do not** re-process \"Hi\" and \"Hello\".\n",
    "    *   We pass `x=\"How are you?\"` and `kv_cache=KV(\"Hi\", \"Hello\")`.\n",
    "    *   The model computes attention for \"How are you?\" attending to the cached \"Hi\" and \"Hello\".\n",
    "\n",
    "In this notebook's loop, `round_id=2` is exactly like **Turn 2**. We feed new tokens (`token_tensors`) but provide the history (`kv_cache`) so the model understands the context without re-computing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ca968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCacheManager:\n",
    "    def __init__(self, max_cache_size: int = 64):\n",
    "        self.cache : List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
    "        self.token_labels : List[str] = [] # To store labels for each token in the cache\n",
    "        self.max_cache_size = max_cache_size\n",
    "\n",
    "    def get_cache(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if not self.cache:\n",
    "            return None\n",
    "        \n",
    "        k = torch.cat([item[0] for item in self.cache], dim=1)  # Concatenate along sequence length\n",
    "        v = torch.cat([item[1] for item in self.cache], dim=1)  # Concatenate along sequence length\n",
    "        return (k, v) # shape of k or v: (batch_size, total_sequence_length, embed_dim)\n",
    "\n",
    "    def update_cache(self, new_kv: Tuple[torch.Tensor, torch.Tensor], tokens : List[str], current_round : int):\n",
    "        self.cache.append(new_kv)\n",
    "        self.token_labels += [f\"Round{current_round}\"] * new_kv[0].size(1)  # Assuming new_kv[0] shape is (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        if len(self.token_labels) > self.max_cache_size:\n",
    "            # Keep only current round tokens if cache is full\n",
    "            # Note: The original logic was trying to filter based on labels. \n",
    "            # Since we append new_kv (current round) at the end, and we want to keep \"Round{current_round}\",\n",
    "            # we can simply keep the last element of the cache if we assume previous rounds are what we want to discard.\n",
    "            \n",
    "            # Simplified logic to avoid tensor unpacking errors from original code\n",
    "            self.cache = [self.cache[-1]] \n",
    "            self.token_labels = [label for label in self.token_labels if label == f\"Round{current_round}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c40d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(prompt : str, vocab : List[str], num_tokens: int = 5) -> List[str]:\n",
    "    return [random.choice(vocab) for _ in range(num_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "decoder = SimpleDecoderBlock(embed_dim=64, num_heads=4).to(device)\n",
    "kv_cache_manager = KVCacheManager(max_cache_size=30)\n",
    "vocab = list(string.ascii_lowercase)  # Example vocabulary\n",
    "\n",
    "for round_id in range(1, 6):\n",
    "    prompt = f\"[Round {round_id}] User Input: write an function\"\n",
    "    tokens = generate_tokens(prompt, vocab)\n",
    "    print(f\"Round {round_id} generated tokens: {' '.join(tokens)}\")\n",
    "\n",
    "    # Simulate token embeddings\n",
    "    token_tensors = torch.stack([torch.randn(64) for _ in tokens]).unsqueeze(0).to(device)  # shape: (1, seq_len, embed_dim)\n",
    "\n",
    "    # Retrieve kv_cache and decode\n",
    "    kv_cache = kv_cache_manager.get_cache()\n",
    "    output, new_kv = decoder(token_tensors, kv_cache=kv_cache, use_cache=True)\n",
    "\n",
    "    if new_kv is not None:\n",
    "        kv_cache_manager.update_cache(new_kv, tokens, current_round=round_id)\n",
    "\n",
    "    summary = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
    "    print(f\"Round {round_id} summary: {summary}\")\n",
    "\n",
    "print(\"\\n=== Final KV Cache State ===\")\n",
    "print(f\"Current token number in cache: {len(kv_cache_manager.token_labels)}\")\n",
    "print(f\"Round labels in cache: {kv_cache_manager.token_labels}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
