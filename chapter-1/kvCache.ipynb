{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import string\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9891377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        super(SimpleDecoderBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv_cache = None, use_cache: bool = True) -> torch.Tensor:\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            # Use cached key and value tensors for efficient decoding\n",
    "            k, v = kv_cache\n",
    "            x_attn, _ = self.self_attn(self.norm1(x), k, v, need_weights=False)\n",
    "        else:\n",
    "            # Compute self-attention normally\n",
    "            x_attn, _ = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), need_weights=False)\n",
    "\n",
    "        x = x + x_attn\n",
    "        x = self.ffn(self.norm2(x))\n",
    "        \n",
    "        if use_cache:\n",
    "            # Update kv_cache with new key and value tensors\n",
    "            return x, (x.clone().detach(), x.clone().detach())\n",
    "\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ca968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCacheManager:\n",
    "    def __init__(self, max_cache_size: int = 64):\n",
    "        self.cache : List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
    "        self.token_labels : List[str] = [] # To store labels for each token in the cache\n",
    "        self.max_cache_size = max_cache_size\n",
    "\n",
    "    def get_cache(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if not self.cache:\n",
    "            return None\n",
    "        \n",
    "        k = torch.cat([item[0] for item in self.cache], dim=1)  # Concatenate along sequence length\n",
    "        v = torch.cat([item[1] for item in self.cache], dim=1)  # Concatenate along sequence length\n",
    "        return (k, v) # shape of k or v: (batch_size, total_sequence_length, embed_dim)\n",
    "\n",
    "    def update_cache(self, new_kv: Tuple[torch.Tensor, torch.Tensor], tokens : List[str], current_round : int):\n",
    "        self.cache.append(new_kv)\n",
    "        self.token_labels += [f\"Round{current_round}\"] * new_kv[0].size(1)  # Assuming new_kv[0] shape is (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        if len(self.token_labels) > self.max_cache_size:\n",
    "            # Keep only current round tokens if cache is full\n",
    "            # Note: The original logic was trying to filter based on labels. \n",
    "            # Since we append new_kv (current round) at the end, and we want to keep \"Round{current_round}\",\n",
    "            # we can simply keep the last element of the cache if we assume previous rounds are what we want to discard.\n",
    "            \n",
    "            # Simplified logic to avoid tensor unpacking errors from original code\n",
    "            self.cache = [self.cache[-1]] \n",
    "            self.token_labels = [label for label in self.token_labels if label == f\"Round{current_round}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c40d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(prompt : str, vocab : List[str], num_tokens: int = 5) -> List[str]:\n",
    "    return [random.choice(vocab) for _ in range(num_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "decoder = SimpleDecoderBlock(embed_dim=64, num_heads=4).to(device)\n",
    "kv_cache_manager = KVCacheManager(max_cache_size=30)\n",
    "vocab = list(string.ascii_lowercase)  # Example vocabulary\n",
    "\n",
    "for round_id in range(1, 6):\n",
    "    prompt = f\"[Round {round_id}] User Input: write an function\"\n",
    "    tokens = generate_tokens(prompt, vocab)\n",
    "    print(f\"Round {round_id} generated tokens: {' '.join(tokens)}\")\n",
    "\n",
    "    # Simulate token embeddings\n",
    "    token_tensors = torch.stack([torch.randn(64) for _ in tokens]).unsqueeze(0).to(device)  # shape: (1, seq_len, embed_dim)\n",
    "\n",
    "    # Retrieve kv_cache and decode\n",
    "    kv_cache = kv_cache_manager.get_cache()\n",
    "    output, new_kv = decoder(token_tensors, kv_cache=kv_cache, use_cache=True)\n",
    "\n",
    "    if new_kv is not None:\n",
    "        kv_cache_manager.update_cache(new_kv, tokens, current_round=round_id)\n",
    "\n",
    "    summary = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
    "    print(f\"Round {round_id} summary: {summary}\")\n",
    "\n",
    "print(\"\\n=== Final KV Cache State ===\")\n",
    "print(f\"Current token number in cache: {len(kv_cache_manager.token_labels)}\")\n",
    "print(f\"Round labels in cache: {kv_cache_manager.token_labels}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
