{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee2f991",
   "metadata": {},
   "source": [
    "# Low-Rank Compensation for Int8 Quantization\n",
    "\n",
    "This notebook demonstrates how to improve Int8 quantization accuracy using Low-Rank Compensation (LoRC) via SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ed534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def generate_weight_matrix(rows: int, cols: int) -> np.ndarray:\n",
    "    # Generates a random weight matrix with values uniformly distributed between -1 and 1\n",
    "    return np.random.uniform(-1, 1, (rows, cols)).astype(np.float32)\n",
    "\n",
    "def calculate_mean_squared_error(original: np.ndarray, reconstructed: np.ndarray) -> float:\n",
    "    # Calculates the Mean Squared Error between the original and reconstructed matrices\n",
    "    return np.mean((original - reconstructed) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6c7ca",
   "metadata": {},
   "source": [
    "### Int8 Quantization\n",
    "\n",
    "Standard symmetric quantization to 8-bit integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_to_int8(matrix: np.ndarray) -> (np.ndarray, float):\n",
    "    # Quantizes the input matrix to int8\n",
    "    scale = np.max(np.abs(matrix)) / 127  # Scale factor for int8 (-128 to 127, signed 8-bit integer)\n",
    "    quantized = np.clip(np.round(matrix / scale), -128, 127).astype(np.int8) # Ensure values fit in int8 range (round and cast)\n",
    "    return quantized, scale\n",
    "\n",
    "def dequantize_from_int8(quantized: np.ndarray, scale: float) -> np.ndarray:\n",
    "    # Dequantizes the int8 matrix back to float32\n",
    "    return quantized.astype(np.float32) * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea57828",
   "metadata": {},
   "source": [
    "### Low-Rank Compensation\n",
    "\n",
    "We use Singular Value Decomposition (SVD) to approximate the *residual error* (Original - Quantized). The goal is to decompose a matrix $M$ into two smaller matrices $A$ and $B$ such that $M \\approx A \\times B$.\n",
    "\n",
    "Here is the step-by-step breakdown:\n",
    "\n",
    "1.  **Perform SVD**:\n",
    "    ```python\n",
    "    U, S, Vt = np.linalg.svd(original, full_matrices=False)\n",
    "    ```\n",
    "    *   This function breaks the `original` matrix into three components: $U$, $\\Sigma$ (represented by `S`), and $V^T$ (`Vt`).\n",
    "    *   Mathematically: $M = U \\cdot \\Sigma \\cdot V^T$.\n",
    "    *   `S` is a 1D array containing the **singular values**, which represent the \"strength\" or importance of each component, sorted from largest to smallest.\n",
    "\n",
    "2.  **Create Matrix A (Left Factor)**:\n",
    "    ```python\n",
    "    A = U[:, :rank] * np.sqrt(S[:rank])\n",
    "    ```\n",
    "    *   `U[:, :rank]`: Takes the first `rank` columns of $U$ (the most important features).\n",
    "    *   `np.sqrt(S[:rank])`: Takes the square root of the top `rank` singular values.\n",
    "    *   **Why square root?** To balance the magnitude between $A$ and $B$, the singular values are split evenly. $A$ gets half the \"weight\" ($\\sqrt{\\Sigma}$).\n",
    "    *   **Result**: $A$ has shape `(rows, rank)`.\n",
    "\n",
    "3.  **Create Matrix B (Right Factor)**:\n",
    "    ```python\n",
    "    B = (np.sqrt(S[:rank])[:, np.newaxis] * Vt[:rank, :])\n",
    "    ```\n",
    "    *   `Vt[:rank, :]`: Takes the first `rank` rows of $V^T$.\n",
    "    *   `np.sqrt(S[:rank])`: The other half of the \"weight\".\n",
    "    *   `[:, np.newaxis]`: Reshapes the 1D array into a column vector so it can be multiplied correctly (broadcasted) across the rows of `Vt`.\n",
    "    *   **Result**: $B$ has shape `(rank, cols)`.\n",
    "\n",
    "**Summary**:\n",
    "When you multiply $A \\times B$, you get $U_{rank} \\cdot \\sqrt{S_{rank}} \\cdot \\sqrt{S_{rank}} \\cdot V^T_{rank} = U_{rank} \\cdot S_{rank} \\cdot V^T_{rank}$. This reconstructs the original matrix using only the most important information, discarding the noise (the smaller singular values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_compensation(original: np.ndarray, rank: int=8) -> np.ndarray:\n",
    "    # The idea is to approximate the original matrix with a low-rank matrix\n",
    "    # In details, we perform SVD and keep only the top 'rank' singular values/vectors\n",
    "    # the shape of original is (m, n)\n",
    "    U, S, Vt = np.linalg.svd(original, full_matrices=False) # the shape of U is (m, m), S is (min(m,n),), Vt is (n, n)\n",
    "\n",
    "    # Get the top 'rank' components\n",
    "    A = U[:, :rank] * np.sqrt(S[:rank])\n",
    "    # Get the corresponding B matrix\n",
    "    B = (np.sqrt(S[:rank])[:, np.newaxis] * Vt[:rank, :])\n",
    "\n",
    "    # Reconstruct the low-rank approximation\n",
    "    compensated = np.dot(A, B)\n",
    "    return compensated, A, B\n",
    "\n",
    "def apply_low_rank_compensation(quantized: np.ndarray, scale: float, compensation: (np.ndarray, np.ndarray)) -> np.ndarray:\n",
    "    # Dequantize the quantized matrix\n",
    "    dequantized = dequantize_from_int8(quantized, scale)\n",
    "    # Apply low-rank compensation\n",
    "    compensated = dequantized + compensation\n",
    "    return compensated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976cbf3",
   "metadata": {},
   "source": [
    "### Why Calculate the Residual?\n",
    "\n",
    "It is crucial to apply the low-rank approximation to the **residual error** ($W - W_{quant}$), not the original matrix.\n",
    "\n",
    "*   **Incorrect Approach**: If we approximate the original matrix ($W \\approx AB$) and add it to the quantized matrix ($W_{quant}$), we get $W_{quant} + AB \\approx 2W$. This doubles the signal magnitude and ruins accuracy.\n",
    "*   **Correct Approach (Residual)**: We want to capture what was *lost* during quantization.\n",
    "    1.  Calculate Residual: $R = W - W_{quant}$.\n",
    "    2.  Approximate Residual: $R \\approx AB$.\n",
    "    3.  Compensate: $W_{final} = W_{quant} + AB \\approx W_{quant} + (W - W_{quant}) = W$.\n",
    "\n",
    "This effectively \"adds back\" the information lost during the quantization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "rows, cols = 256, 256\n",
    "original_matrix = generate_weight_matrix(rows, cols)\n",
    "\n",
    "# Int8 Quantization\n",
    "quantized_int8, scale_int8 = quantize_to_int8(original_matrix)\n",
    "reconstructed_int8 = dequantize_from_int8(quantized_int8, scale_int8)\n",
    "mse_int8 = calculate_mean_squared_error(original_matrix, reconstructed_int8)\n",
    "print(f\"Int8 Quantization MSE: {mse_int8}\")\n",
    "\n",
    "# Generate low-rank compensation\n",
    "residual = original_matrix - reconstructed_int8\n",
    "compensation, A, B = low_rank_compensation(residual, rank=8)\n",
    "\n",
    "# Apply Low-Rank Compensation\n",
    "compensated_reconstruction = apply_low_rank_compensation(quantized_int8, scale_int8, compensation)\n",
    "mse_compensated = calculate_mean_squared_error(original_matrix, compensated_reconstruction)\n",
    "print(f\"Int8 with Low-Rank Compensation MSE: {mse_compensated}\")\n",
    "\n",
    "print(f\"Original Matrix Sample:\\n{original_matrix[:5, :5]}\\n\")\n",
    "print(f\"INT8 Quantized Sample:\\n{quantized_int8[:5, :5]}\\n\")\n",
    "print(f\"INT8 Dequantized Sample:\\n{reconstructed_int8[:5, :5]}\\n\")\n",
    "print(f\"INT8 MSE: {mse_int8}\\n\\n\")\n",
    "print(f\"Compensated Reconstruction Sample:\\n{compensated_reconstruction[:5, :5]}\\n\")\n",
    "print(f\"Compensated MSE: {mse_compensated}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
