{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nncliff/qwen-32B/blob/main/chapter-4/ipynb/locoInt8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee2f991",
      "metadata": {
        "id": "3ee2f991"
      },
      "source": [
        "# Low-Rank Compensation for Int8 Quantization\n",
        "\n",
        "This notebook demonstrates how to improve Int8 quantization accuracy using Low-Rank Compensation (LoRC) via SVD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7e1ed534",
      "metadata": {
        "id": "7e1ed534"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def generate_weight_matrix(rows: int, cols: int) -> np.ndarray:\n",
        "    # Generates a random weight matrix with values uniformly distributed between -1 and 1\n",
        "    return np.random.uniform(-1, 1, (rows, cols)).astype(np.float32)\n",
        "\n",
        "def calculate_mean_squared_error(original: np.ndarray, reconstructed: np.ndarray) -> float:\n",
        "    # Calculates the Mean Squared Error between the original and reconstructed matrices\n",
        "    return np.mean((original - reconstructed) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d6c7ca",
      "metadata": {
        "id": "18d6c7ca"
      },
      "source": [
        "### Int8 Quantization\n",
        "\n",
        "Standard symmetric quantization to 8-bit integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8598f94b",
      "metadata": {
        "id": "8598f94b"
      },
      "outputs": [],
      "source": [
        "def quantize_to_int8(matrix: np.ndarray) -> (np.ndarray, float):\n",
        "    # Quantizes the input matrix to int8\n",
        "    scale = np.max(np.abs(matrix)) / 127  # Scale factor for int8 (-128 to 127, signed 8-bit integer)\n",
        "    quantized = np.clip(np.round(matrix / scale), -128, 127).astype(np.int8) # Ensure values fit in int8 range (round and cast)\n",
        "    return quantized, scale\n",
        "\n",
        "def dequantize_from_int8(quantized: np.ndarray, scale: float) -> np.ndarray:\n",
        "    # Dequantizes the int8 matrix back to float32\n",
        "    return quantized.astype(np.float32) * scale"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea57828",
      "metadata": {
        "id": "fea57828"
      },
      "source": [
        "### Low-Rank Compensation\n",
        "\n",
        "We use Singular Value Decomposition (SVD) to approximate the *residual error* (Original - Quantized). The goal is to decompose a matrix $M$ into two smaller matrices $A$ and $B$ such that $M \\approx A \\times B$.\n",
        "\n",
        "Here is the step-by-step breakdown:\n",
        "\n",
        "1.  **Perform SVD**:\n",
        "    ```python\n",
        "    U, S, Vt = np.linalg.svd(original, full_matrices=False)\n",
        "    ```\n",
        "    *   This function breaks the `original` matrix into three components: $U$, $\\Sigma$ (represented by `S`), and $V^T$ (`Vt`).\n",
        "    *   Mathematically: $M = U \\cdot \\Sigma \\cdot V^T$.\n",
        "    *   `S` is a 1D array containing the **singular values**, which represent the \"strength\" or importance of each component, sorted from largest to smallest.\n",
        "\n",
        "2.  **Create Matrix A (Left Factor)**:\n",
        "    ```python\n",
        "    A = U[:, :rank] * np.sqrt(S[:rank])\n",
        "    ```\n",
        "    *   `U[:, :rank]`: Takes the first `rank` columns of $U$ (the most important features).\n",
        "    *   `np.sqrt(S[:rank])`: Takes the square root of the top `rank` singular values.\n",
        "    *   **Why square root?** To balance the magnitude between $A$ and $B$, the singular values are split evenly. $A$ gets half the \"weight\" ($\\sqrt{\\Sigma}$).\n",
        "    *   **Result**: $A$ has shape `(rows, rank)`.\n",
        "\n",
        "3.  **Create Matrix B (Right Factor)**:\n",
        "    ```python\n",
        "    B = (np.sqrt(S[:rank])[:, np.newaxis] * Vt[:rank, :])\n",
        "    ```\n",
        "    *   `Vt[:rank, :]`: Takes the first `rank` rows of $V^T$.\n",
        "    *   `np.sqrt(S[:rank])`: The other half of the \"weight\".\n",
        "    *   `[:, np.newaxis]`: Reshapes the 1D array into a column vector so it can be multiplied correctly (broadcasted) across the rows of `Vt`.\n",
        "    *   **Result**: $B$ has shape `(rank, cols)`.\n",
        "\n",
        "**Summary**:\n",
        "When you multiply $A \\times B$, you get $U_{rank} \\cdot \\sqrt{S_{rank}} \\cdot \\sqrt{S_{rank}} \\cdot V^T_{rank} = U_{rank} \\cdot S_{rank} \\cdot V^T_{rank}$. This reconstructs the original matrix using only the most important information, discarding the noise (the smaller singular values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8148fe9b",
      "metadata": {
        "id": "8148fe9b"
      },
      "outputs": [],
      "source": [
        "def low_rank_compensation(original: np.ndarray, rank: int=8) -> np.ndarray:\n",
        "    # The idea is to approximate the original matrix with a low-rank matrix\n",
        "    # In details, we perform SVD and keep only the top 'rank' singular values/vectors\n",
        "    # the shape of original is (m, n)\n",
        "    U, S, Vt = np.linalg.svd(original, full_matrices=False) # the shape of U is (m, m), S is (min(m,n),), Vt is (n, n)\n",
        "\n",
        "    # Get the top 'rank' components\n",
        "    A = U[:, :rank] * np.sqrt(S[:rank])\n",
        "    # Get the corresponding B matrix\n",
        "    B = (np.sqrt(S[:rank])[:, np.newaxis] * Vt[:rank, :])\n",
        "\n",
        "    # Reconstruct the low-rank approximation\n",
        "    compensated = np.dot(A, B)\n",
        "    return compensated, A, B\n",
        "\n",
        "def apply_low_rank_compensation(quantized: np.ndarray, scale: float, compensation: (np.ndarray, np.ndarray)) -> np.ndarray:\n",
        "    # Dequantize the quantized matrix\n",
        "    dequantized = dequantize_from_int8(quantized, scale)\n",
        "    # Apply low-rank compensation\n",
        "    compensated = dequantized + compensation\n",
        "    return compensated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a976cbf3",
      "metadata": {
        "id": "a976cbf3"
      },
      "source": [
        "### Why Calculate the Residual?\n",
        "\n",
        "It is crucial to apply the low-rank approximation to the **residual error** ($W - W_{quant}$), not the original matrix.\n",
        "\n",
        "*   **Incorrect Approach**: If we approximate the original matrix ($W \\approx AB$) and add it to the quantized matrix ($W_{quant}$), we get $W_{quant} + AB \\approx 2W$. This doubles the signal magnitude and ruins accuracy.\n",
        "*   **Correct Approach (Residual)**: We want to capture what was *lost* during quantization.\n",
        "    1.  Calculate Residual: $R = W - W_{quant}$.\n",
        "    2.  Approximate Residual: $R \\approx AB$.\n",
        "    3.  Compensate: $W_{final} = W_{quant} + AB \\approx W_{quant} + (W - W_{quant}) = W$.\n",
        "\n",
        "This effectively \"adds back\" the information lost during the quantization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c531e808",
      "metadata": {
        "id": "c531e808",
        "outputId": "6174a0c2-61fb-498a-bed9-68a1e7441d08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int8 Quantization MSE: 5.1503930080798455e-06\n",
            "Int8 with Low-Rank Compensation MSE: 4.576992978400085e-06\n",
            "Original Matrix Sample:\n",
            "[[-0.25091976  0.90142864  0.4639879   0.19731697 -0.6879627 ]\n",
            " [ 0.87230957  0.3920596   0.14012234 -0.805647    0.23001446]\n",
            " [ 0.86545694  0.7321278  -0.90956265 -0.94726604 -0.24707326]\n",
            " [-0.3158365   0.6425146  -0.77873653  0.6929046  -0.74502265]\n",
            " [ 0.888533   -0.05157157  0.72408533  0.6890988  -0.36179906]]\n",
            "\n",
            "INT8 Quantized Sample:\n",
            "[[ -32  114   59   25  -87]\n",
            " [ 111   50   18 -102   29]\n",
            " [ 110   93 -116 -120  -31]\n",
            " [ -40   82  -99   88  -95]\n",
            " [ 113   -7   92   88  -46]]\n",
            "\n",
            "INT8 Dequantized Sample:\n",
            "[[-0.2519657   0.89762783  0.46456176  0.1968482  -0.6850318 ]\n",
            " [ 0.87400603  0.3936964   0.14173071 -0.8031407   0.22834392]\n",
            " [ 0.8661321   0.7322753  -0.9133757  -0.94487137 -0.24409178]\n",
            " [-0.31495714  0.6456621  -0.7795189   0.69290566 -0.74802315]\n",
            " [ 0.8897539  -0.0551175   0.7244014   0.69290566 -0.3622007 ]]\n",
            "\n",
            "INT8 MSE: 5.1503930080798455e-06\n",
            "\n",
            "\n",
            "Compensated Reconstruction Sample:\n",
            "[[-0.25158548  0.8981052   0.46400625  0.19703662 -0.6847261 ]\n",
            " [ 0.8735853   0.39272827  0.14172189 -0.80322593  0.22752227]\n",
            " [ 0.866255    0.73227614 -0.91320485 -0.94496024 -0.2437592 ]\n",
            " [-0.3150963   0.64497495 -0.77924263  0.6932134  -0.74761325]\n",
            " [ 0.8903919  -0.0542714   0.7232213   0.6923524  -0.36175779]]\n",
            "\n",
            "Compensated MSE: 4.576992978400085e-06\n",
            "\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "rows, cols = 256, 256\n",
        "original_matrix = generate_weight_matrix(rows, cols)\n",
        "\n",
        "# Int8 Quantization\n",
        "quantized_int8, scale_int8 = quantize_to_int8(original_matrix)\n",
        "reconstructed_int8 = dequantize_from_int8(quantized_int8, scale_int8)\n",
        "mse_int8 = calculate_mean_squared_error(original_matrix, reconstructed_int8)\n",
        "print(f\"Int8 Quantization MSE: {mse_int8}\")\n",
        "\n",
        "# Generate low-rank compensation\n",
        "residual = original_matrix - reconstructed_int8\n",
        "compensation, A, B = low_rank_compensation(residual, rank=8)\n",
        "\n",
        "# Apply Low-Rank Compensation\n",
        "compensated_reconstruction = apply_low_rank_compensation(quantized_int8, scale_int8, compensation)\n",
        "mse_compensated = calculate_mean_squared_error(original_matrix, compensated_reconstruction)\n",
        "print(f\"Int8 with Low-Rank Compensation MSE: {mse_compensated}\")\n",
        "\n",
        "print(f\"Original Matrix Sample:\\n{original_matrix[:5, :5]}\\n\")\n",
        "print(f\"INT8 Quantized Sample:\\n{quantized_int8[:5, :5]}\\n\")\n",
        "print(f\"INT8 Dequantized Sample:\\n{reconstructed_int8[:5, :5]}\\n\")\n",
        "print(f\"INT8 MSE: {mse_int8}\\n\\n\")\n",
        "print(f\"Compensated Reconstruction Sample:\\n{compensated_reconstruction[:5, :5]}\\n\")\n",
        "print(f\"Compensated MSE: {mse_compensated}\\n\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}